{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jintubhuyan-2000/ML-XAI_ForestFire/blob/main/RegionTransfer_California_FinalRevised2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown --quiet\n",
        "\n",
        "# Use gdown to download the folder\n",
        "!gdown \"https://drive.google.com/drive/u/0/folders/1ZPvCvLcy68RGEFKIA1elHrDdzMSXKZ8a\" --folder"
      ],
      "metadata": {
        "id": "2bOUvWn2HTXI",
        "outputId": "1e07fc72-1fa7-4137-d5a2-185d34a1ed94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2bOUvWn2HTXI",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1q0JYs5vwwDkEN3gVQBc4-7oAKGx6J189 AccuracyStats_2025_grassland.csv\n",
            "Processing file 1wgWqbfJ5_qMgH2TgqbIRfgBseKvpUFJ2 ConfusionMatrix_2025_grassland.csv\n",
            "Processing file 1CYhNkpQr_9q8G947aTYd41X26B_yzGh1 FireProbStats_PerDistrict_grassland.csv\n",
            "Processing file 17GP1DDg1USLDjZAv-BBH-QSJlQjzAPSu RF_RegionTransfer_Test_grassland.csv\n",
            "Processing file 1Y08p-xgwuWZNR20Mx2gbB27MZNoUZH_w RF_SpatialCV_Test_grassland.csv\n",
            "Processing file 19xIgV46X3CI7f1txivigEZO7yioXOFxF RF_TemporalSplit_Test2025_grassland.csv\n",
            "Processing file 1I4JLP1qu5AuP80uHISDV_asbihnH9pYr TestPoints_Predictors_grassland.csv\n",
            "Processing file 1drsd-aZRdiKnNblLniHCuN7sGdc69dFG TrainPoints_Predictors_grassland.csv\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1q0JYs5vwwDkEN3gVQBc4-7oAKGx6J189\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KX8PgDM6HUtO"
      },
      "id": "KX8PgDM6HUtO",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8e889d0a-8210-4ca6-8dd5-50862ae85fb0",
      "metadata": {
        "id": "8e889d0a-8210-4ca6-8dd5-50862ae85fb0",
        "outputId": "dbdc27ad-1ad3-40e5-fd86-0eb3b3372d40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train CSV: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Validation Datasets/TrainPoints_Predictors_grassland.csv\n",
            "Test CSV: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Validation Datasets/RF_SpatialCV_Test_grassland.csv\n",
            "Saved per-run metrics: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Results/results_region_transfer_V5/metrics_per_run.csv\n",
            "Saved per-run Top-K captures: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Results/results_region_transfer_V5/topk_per_run.csv\n",
            "No 'stratum' column found in test CSV; skipping aggregation by stratum.\n",
            "Summary report written: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Results/results_region_transfer_V5/summary_report_multi_run.txt\n",
            "\n",
            "All done. Check results in: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Results/results_region_transfer_V5\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Wildfire RF analysis with multiple runs & uncertainty.\n",
        "Generates mean Â± SD curves for ROC, PR, Calibration, Top-K.\n",
        "Saves per-run metrics, Top-K captures, aggregated metrics by stratum, and summary.\n",
        "\"\"\"\n",
        "\n",
        "import os, glob, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
        "                             roc_curve, precision_recall_curve, f1_score, precision_score, recall_score)\n",
        "from sklearn.calibration import calibration_curve\n",
        "import joblib\n",
        "\n",
        "# Optional SHAP\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "    warnings.warn('shap not available. Install shap to compute SHAP plots.')\n",
        "\n",
        "# ----------------------\n",
        "# Paths\n",
        "# ----------------------\n",
        "BASE_DIR = r\"/content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Validation Datasets\"\n",
        "RESULTS_DIR = os.path.join('/content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Results/results_region_transfer_V5')\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "OUT_DIR = RESULTS_DIR\n",
        "METRICS_PER_RUN_CSV = os.path.join(OUT_DIR, \"metrics_per_run.csv\")\n",
        "METRICS_AGG_CSV = os.path.join(OUT_DIR, \"metrics_aggregated_by_stratum.csv\")\n",
        "TOPK_PER_RUN_CSV = os.path.join(OUT_DIR, \"topk_per_run.csv\")\n",
        "TOPK_AGG_CSV = os.path.join(OUT_DIR, \"topk_aggregated.csv\")\n",
        "SUMMARY_TXT = os.path.join(OUT_DIR, \"summary_report_multi_run.txt\")\n",
        "\n",
        "# ----------------------\n",
        "# CSV detection\n",
        "# ----------------------\n",
        "csv_files = glob.glob(os.path.join(BASE_DIR, '**', '*.csv'), recursive=True)\n",
        "def find_candidate(files, keywords):\n",
        "    keywords = [k.lower() for k in keywords]\n",
        "    for f in files:\n",
        "        name = os.path.basename(f).lower()\n",
        "        if any(k in name for k in keywords):\n",
        "            return f\n",
        "    return None\n",
        "\n",
        "train_csv = find_candidate(csv_files, ['train'])\n",
        "test_csv = find_candidate(csv_files, ['test'])\n",
        "if not train_csv or not test_csv:\n",
        "    csv_sizes = sorted(csv_files, key=os.path.getsize, reverse=True)\n",
        "    if len(csv_sizes) >= 2:\n",
        "        train_csv = csv_sizes[0] if not train_csv else train_csv\n",
        "        test_csv = csv_sizes[1] if not test_csv else test_csv\n",
        "\n",
        "print('Train CSV:', train_csv)\n",
        "print('Test CSV:', test_csv)\n",
        "\n",
        "train = pd.read_csv(train_csv)\n",
        "test = pd.read_csv(test_csv)\n",
        "\n",
        "# Clean column names\n",
        "train.columns = [c.strip() for c in train.columns]\n",
        "test.columns = [c.strip() for c in test.columns]\n",
        "\n",
        "# Target column\n",
        "possible_targets = ['class', 'y', 'label', 'fire', 'is_fire']\n",
        "train_cols_low = [c.lower() for c in train.columns]\n",
        "target_col = next((train.columns[i] for i,t in enumerate(train_cols_low) if t in possible_targets), None)\n",
        "if target_col is None:\n",
        "    raise ValueError(\"Target column not found\")\n",
        "\n",
        "train[target_col] = train[target_col].astype(int)\n",
        "if target_col in test.columns:\n",
        "    test[target_col] = test[target_col].astype(int)\n",
        "\n",
        "y_train = train[target_col].values\n",
        "y_true = test[target_col].values if target_col in test.columns else None\n",
        "\n",
        "# Predictor columns\n",
        "predictors = [c for c in train.select_dtypes(include=[np.number]).columns\n",
        "              if c != target_col and 'id' not in c.lower()]\n",
        "X_train = train[predictors].fillna(-999)\n",
        "X_test = test[predictors].fillna(-999)\n",
        "\n",
        "# ----------------------\n",
        "# Multiple runs\n",
        "# ----------------------\n",
        "NUM_RUNS = 10\n",
        "SEEDS = list(range(42, 42+NUM_RUNS))\n",
        "\n",
        "# Storage\n",
        "roc_list, pr_list, cal_list, topk_list = [], [], [], []\n",
        "metrics_all = []\n",
        "y_prob_runs = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=seed, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    joblib.dump(rf, os.path.join(RESULTS_DIR, f'rf_model_{seed}.joblib'))\n",
        "\n",
        "    y_prob = rf.predict_proba(X_test)[:,1]\n",
        "    y_prob_runs.append(y_prob)\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'seed': seed,\n",
        "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
        "        'pr_auc': average_precision_score(y_true, y_prob),\n",
        "        'brier': brier_score_loss(y_true, y_prob),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred)\n",
        "    }\n",
        "    metrics_all.append(metrics)\n",
        "\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_list.append(np.interp(np.linspace(0,1,100), fpr, tpr))\n",
        "\n",
        "    # PR\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "    pr_list.append(np.interp(np.linspace(0,1,100), recall[::-1], precision[::-1]))\n",
        "\n",
        "    # Calibration\n",
        "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
        "    cal_list.append(np.interp(np.linspace(0,1,10), prob_pred, prob_true))\n",
        "\n",
        "    # Top-K capture\n",
        "    df_test = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob}).sort_values('y_prob', ascending=False)\n",
        "    total_pos = df_test['y_true'].sum()\n",
        "    topk_capture = [df_test.iloc[:int(np.ceil(len(df_test)*f))]['y_true'].sum()/total_pos\n",
        "                    for f in np.linspace(0.01,1,100)]\n",
        "    topk_list.append(topk_capture)\n",
        "\n",
        "# Convert to arrays\n",
        "roc_arr = np.array(roc_list)\n",
        "pr_arr = np.array(pr_list)\n",
        "cal_arr = np.array(cal_list)\n",
        "topk_arr = np.array(topk_list)\n",
        "metrics_df = pd.DataFrame(metrics_all)\n",
        "\n",
        "# ----------------- Plotting with ribbons -----------------\n",
        "ROC_FPR_GRID = np.linspace(0,1,100)\n",
        "PR_RECALL_GRID = np.linspace(0,1,100)\n",
        "TOPK_PERCENTS = np.linspace(0.01,1,100)\n",
        "CAL_PROB_BINS = np.linspace(0,1,11)  # 10 bins\n",
        "\n",
        "# ---------- ROC Curve ----------\n",
        "mean_tpr = np.nanmean(roc_arr, axis=0)\n",
        "sd_tpr = np.nanstd(roc_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(ROC_FPR_GRID, mean_tpr, label=f\"Mean ROC (n={NUM_RUNS})\")\n",
        "plt.fill_between(ROC_FPR_GRID, mean_tpr - sd_tpr, mean_tpr + sd_tpr, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve (mean Â± SD)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'roc_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ---------- PR Curve ----------\n",
        "mean_prec = np.nanmean(pr_arr, axis=0)\n",
        "sd_prec = np.nanstd(pr_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(PR_RECALL_GRID, mean_prec, label=f\"Mean PR (n={NUM_RUNS})\")\n",
        "plt.fill_between(PR_RECALL_GRID, mean_prec - sd_prec, mean_prec + sd_prec, alpha=0.2)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (mean Â± SD)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'pr_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ---------- Top-K Capture ----------\n",
        "mean_topk = np.nanmean(topk_arr, axis=0)\n",
        "sd_topk = np.nanstd(topk_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(TOPK_PERCENTS, mean_topk, marker='o', label='Mean Top-K capture')\n",
        "plt.fill_between(TOPK_PERCENTS, mean_topk - sd_topk, mean_topk + sd_topk, alpha=0.2)\n",
        "plt.xlabel('Top-k percent of highest risk area')\n",
        "plt.ylabel('Fraction of fires captured')\n",
        "plt.title('Top-K Capture (mean Â± SD)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'topk_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ---------- Reliability / Calibration ----------\n",
        "cal_bin_centers = (CAL_PROB_BINS[:-1] + CAL_PROB_BINS[1:]) / 2.0\n",
        "mean_bin_obs = np.nanmean(cal_arr, axis=0)\n",
        "sd_bin_obs = np.nanstd(cal_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(cal_bin_centers, mean_bin_obs, marker='o', label='Mean calibration')\n",
        "plt.fill_between(cal_bin_centers, mean_bin_obs - sd_bin_obs, mean_bin_obs + sd_bin_obs, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', label='Perfect')\n",
        "plt.xlabel('Predicted probability (bin center)')\n",
        "plt.ylabel('Observed frequency')\n",
        "plt.title('Reliability Diagram (mean Â± SD)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'reliability_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ----------------- Save per-run metrics -----------------\n",
        "metrics_df.to_csv(METRICS_PER_RUN_CSV, index=False)\n",
        "topk_df = pd.DataFrame(topk_arr, columns=TOPK_PERCENTS)\n",
        "topk_df['seed'] = SEEDS\n",
        "topk_df.to_csv(TOPK_PER_RUN_CSV, index=False)\n",
        "print(f\"Saved per-run metrics: {METRICS_PER_RUN_CSV}\")\n",
        "print(f\"Saved per-run Top-K captures: {TOPK_PER_RUN_CSV}\")\n",
        "\n",
        "# ----------------- Aggregate metrics by stratum -----------------\n",
        "if 'stratum' in test.columns:\n",
        "    stratum_vals = test['stratum'].unique()\n",
        "    agg_metrics_list = []\n",
        "    agg_topk_list = []\n",
        "\n",
        "    for s in stratum_vals:\n",
        "        idx = test[test['stratum']==s].index\n",
        "        metrics_sub, topk_sub = [], []\n",
        "\n",
        "        for seed_i, seed in enumerate(SEEDS):\n",
        "            y_prob_sub = y_prob_runs[seed_i][idx]\n",
        "            y_true_sub = y_true[idx]\n",
        "            y_pred_sub = (y_prob_sub >= 0.5).astype(int)\n",
        "\n",
        "            metrics_sub.append({\n",
        "                'seed': seed,\n",
        "                'stratum': s,\n",
        "                'roc_auc': roc_auc_score(y_true_sub, y_prob_sub),\n",
        "                'pr_auc': average_precision_score(y_true_sub, y_prob_sub),\n",
        "                'brier': brier_score_loss(y_true_sub, y_prob_sub),\n",
        "                'f1': f1_score(y_true_sub, y_pred_sub),\n",
        "                'precision': precision_score(y_true_sub, y_pred_sub),\n",
        "                'recall': recall_score(y_true_sub, y_pred_sub)\n",
        "            })\n",
        "\n",
        "            df_sub = pd.DataFrame({'y_true': y_true_sub, 'y_prob': y_prob_sub}).sort_values('y_prob', ascending=False)\n",
        "            total_pos_sub = df_sub['y_true'].sum()\n",
        "            topk_capture_sub = [df_sub.iloc[:int(np.ceil(len(df_sub)*f))]['y_true'].sum()/total_pos_sub\n",
        "                                for f in TOPK_PERCENTS]\n",
        "            topk_sub.append(topk_capture_sub)\n",
        "\n",
        "        metrics_sub_df = pd.DataFrame(metrics_sub)\n",
        "        agg_metrics_list.append(metrics_sub_df.groupby('stratum').agg(['mean','std']))\n",
        "\n",
        "        topk_sub_arr = np.array(topk_sub)\n",
        "        topk_mean = np.nanmean(topk_sub_arr, axis=0)\n",
        "        topk_sd = np.nanstd(topk_sub_arr, axis=0, ddof=1)\n",
        "        agg_topk_list.append(pd.DataFrame({'stratum': s, 'topk_percent': TOPK_PERCENTS,\n",
        "                                          'mean_capture': topk_mean, 'sd_capture': topk_sd}))\n",
        "\n",
        "    pd.concat(agg_metrics_list).to_csv(METRICS_AGG_CSV)\n",
        "    pd.concat(agg_topk_list).to_csv(TOPK_AGG_CSV, index=False)\n",
        "    print(f\"Saved aggregated metrics by stratum: {METRICS_AGG_CSV}\")\n",
        "    print(f\"Saved aggregated Top-K by stratum: {TOPK_AGG_CSV}\")\n",
        "else:\n",
        "    print(\"No 'stratum' column found in test CSV; skipping aggregation by stratum.\")\n",
        "\n",
        "# ----------------- Summary -----------------\n",
        "with open(SUMMARY_TXT,'w') as fh:\n",
        "    fh.write(f'Random Forest multiple runs evaluation ({NUM_RUNS} runs)\\n')\n",
        "    fh.write('===============================\\n')\n",
        "    fh.write(f'Train CSV: {train_csv}\\n')\n",
        "    fh.write(f'Test CSV: {test_csv}\\n')\n",
        "    fh.write(f'Seed list: {SEEDS}\\n\\n')\n",
        "    fh.write('Metrics (mean Â± SD):\\n')\n",
        "    mean_metrics = metrics_df.mean()\n",
        "    sd_metrics = metrics_df.std()\n",
        "    for col in metrics_df.columns:\n",
        "        if col != 'seed':\n",
        "            fh.write(f' - {col}: {mean_metrics[col]:.4f} Â± {sd_metrics[col]:.4f}\\n')\n",
        "    fh.write('\\nAll plots saved in the results directory.\\n')\n",
        "print(f\"Summary report written: {SUMMARY_TXT}\")\n",
        "\n",
        "print('\\nAll done. Check results in:', RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "67dedc0a-3b02-4102-97cc-748a046dc888",
      "metadata": {
        "id": "67dedc0a-3b02-4102-97cc-748a046dc888",
        "outputId": "c6e4a170-8251-4fa3-8814-1a768f81e2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CSV: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Validation Datasets/RF_SpatialCV_Test_grassland.csv\n",
            "Detected columns -> label: class , prob: classification\n",
            "No stratum column detected - will evaluate overall only.\n",
            "Running 10 runs with seeds: [1281540326, 1005233768, 2011547310, 1367058049, 1542280147, 90017157, 581114276, 1258272007, 2134214070, 1923482161]\n",
            "Region-transfer / Temporal-style Multi-run Evaluation\n",
            "File: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Validation Datasets/RF_SpatialCV_Test_grassland.csv\n",
            "Total original samples: 2462, Overall positive rate: 0.8115\n",
            "Sampling method: stratified_bootstrap\n",
            "Number of runs (K): 10\n",
            "Base seed (document for reproducibility): 20250908\n",
            "Seeds used: [1281540326, 1005233768, 2011547310, 1367058049, 1542280147, 90017157, 581114276, 1258272007, 2134214070, 1923482161]\n",
            "\n",
            "Metrics per-run saved to: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/metrics_per_run.csv\n",
            "Aggregated metrics saved to: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/metrics_aggregated_by_stratum.csv (mean Â± sd Â± 95%CI)\n",
            "Top-K per-run saved to: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/topk_per_run.csv\n",
            "Top-K aggregated saved to: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/topk_aggregated.csv\n",
            "ROC plot (mean Â± SD): /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/roc_curve_mean_sd.png\n",
            "PR plot (mean Â± SD): /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/pr_curve_mean_sd.png\n",
            "Reliability plot (mean Â± SD): /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/reliability_mean_sd.png\n",
            "Top-K plot (mean Â± SD): /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5/topk_curve_mean_sd.png\n",
            "Outputs written to: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5\n"
          ]
        }
      ],
      "source": [
        "# Wildfire RF Evaluation & SHAP Analysis (multi-run + uncertainty)\n",
        "# Paste into a Jupyter cell. Requires: pandas, numpy, matplotlib, scikit-learn\n",
        "# Optional: shap (pip install shap) for SHAP outputs.\n",
        "# Outputs: metrics CSV (per-run + aggregated), topk CSV, curves, summary text.\n",
        "# Author: adapted for multi-run uncertainty & reproducibility\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, roc_curve,\n",
        "    average_precision_score, precision_recall_curve,\n",
        "    brier_score_loss, f1_score, precision_score, recall_score\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# Optional SHAP\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "search_dirs = [\n",
        "    r\"/content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland/Validation Datasets\",\n",
        "]\n",
        "OUT_DIR = Path(r\"/content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/grassland\\Results\\results_region_transfer_V5\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# resampling / reproducibility settings\n",
        "K_RUNS = 10\n",
        "BASE_SEED = 20250908\n",
        "SAMPLING_METHOD = \"stratified_bootstrap\"\n",
        "TEST_SAMPLE_FRACTION = 1.0\n",
        "\n",
        "# Settings for curves / interpolation\n",
        "ROC_FPR_GRID = np.linspace(0,1,200)\n",
        "PR_RECALL_GRID = np.linspace(0,1,200)\n",
        "CAL_PROB_BINS = np.linspace(0.0, 1.0, 11)\n",
        "TOPK_PERCENTS = [1,5,10,20,30,40,50]\n",
        "\n",
        "# Filenames\n",
        "METRICS_PER_RUN_CSV = OUT_DIR / \"metrics_per_run.csv\"\n",
        "METRICS_AGG_CSV = OUT_DIR / \"metrics_aggregated_by_stratum.csv\"\n",
        "TOPK_PER_RUN_CSV = OUT_DIR / \"topk_per_run.csv\"\n",
        "TOPK_AGG_CSV = OUT_DIR / \"topk_aggregated.csv\"\n",
        "SUMMARY_TXT = OUT_DIR / \"summary_report_multi_run.txt\"\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def find_candidate_csv(dirs):\n",
        "    candidates = []\n",
        "    for d in dirs:\n",
        "        for p in Path(d).rglob(\"*.csv\"):\n",
        "            candidates.append(p)\n",
        "    if not candidates:\n",
        "        return None\n",
        "    prioritized = [p for p in candidates if any(k in p.name.lower() for k in (\"test2025\",\"test_2025\",\"rf_regiontransfer\",\"rf_spatialcv_test\",\"rf_region_transfer_test\",\"rf_spatialcv_test\",\"rf_\"))]\n",
        "    if prioritized:\n",
        "        return prioritized[0]\n",
        "    prioritized = [p for p in candidates if any(k in p.name.lower() for k in (\"test\",\"2025\",\"regiontransfer\",\"region_transfer\"))]\n",
        "    if prioritized:\n",
        "        return prioritized[0]\n",
        "    return candidates[0]\n",
        "\n",
        "def topk_capture(y_true, y_prob, ks=TOPK_PERCENTS):\n",
        "    out = []\n",
        "    order = np.argsort(-y_prob)\n",
        "    y_true_sorted = np.array(y_true)[order]\n",
        "    total_pos = float(y_true_sorted.sum())\n",
        "    n = len(y_true_sorted)\n",
        "    for k in ks:\n",
        "        frac = k / 100.0\n",
        "        top_n = max(1, int(math.ceil(n * frac)))\n",
        "        captured = int(y_true_sorted[:top_n].sum())\n",
        "        capture_rate = (captured / total_pos) if total_pos > 0 else np.nan\n",
        "        out.append({\n",
        "            \"top_%\": k,\n",
        "            \"top_n\": top_n,\n",
        "            \"pos_captured_count\": captured,\n",
        "            \"pos_captured_frac\": capture_rate\n",
        "        })\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "def interpolate_curve(x, y, x_grid):\n",
        "    xp = np.clip(x, 0.0, 1.0)\n",
        "    yp = np.clip(y, 0.0, 1.0)\n",
        "    xp_unique, idx = np.unique(xp, return_index=True)\n",
        "    yp_unique = yp[idx]\n",
        "    if len(xp_unique) < 2:\n",
        "        return np.full_like(x_grid, yp_unique[0] if len(yp_unique)>0 else np.nan)\n",
        "    return np.interp(x_grid, xp_unique, yp_unique)\n",
        "\n",
        "# ----------------- Locate CSV -----------------\n",
        "csv_path = find_candidate_csv(search_dirs)\n",
        "if csv_path is None:\n",
        "    raise FileNotFoundError(f\"No CSV found in {search_dirs}. Place your exported CSV(s) in one of those folders.\")\n",
        "print(\"Using CSV:\", csv_path)\n",
        "\n",
        "# ----------------- Load & detect columns -----------------\n",
        "df_orig = pd.read_csv(csv_path)\n",
        "cols = list(df_orig.columns)\n",
        "y_true_col = None\n",
        "y_prob_col = None\n",
        "\n",
        "for c in cols:\n",
        "    lc = c.lower().strip()\n",
        "    if lc in ('class','label','y','ground_truth','is_fire','fire') and y_true_col is None:\n",
        "        y_true_col = c\n",
        "    if lc in ('classification','probability','prob','pred','pred_prob','probability_1') and y_prob_col is None:\n",
        "        y_prob_col = c\n",
        "# fallback heuristics\n",
        "if y_true_col is None:\n",
        "    for c in cols:\n",
        "        if 'class' in c.lower() or c.lower().startswith('label'):\n",
        "            y_true_col = c\n",
        "            break\n",
        "if y_prob_col is None:\n",
        "    for c in cols:\n",
        "        if any(k in c.lower() for k in ('prob','classification','pred')):\n",
        "            y_prob_col = c\n",
        "            break\n",
        "if y_true_col is None or y_prob_col is None:\n",
        "    raise ValueError(f\"Could not auto-detect required columns. Found: {cols}\")\n",
        "print(\"Detected columns -> label:\", y_true_col, \", prob:\", y_prob_col)\n",
        "\n",
        "# optional stratum column detection\n",
        "stratum_col = None\n",
        "for candidate in ['landcover','land_cover','lc','class_name','stratum','habitat','lcc']:\n",
        "    if candidate in [c.lower() for c in cols]:\n",
        "        for c in cols:\n",
        "            if c.lower()==candidate:\n",
        "                stratum_col = c\n",
        "                break\n",
        "        break\n",
        "if stratum_col:\n",
        "    print(\"Detected stratum column:\", stratum_col)\n",
        "else:\n",
        "    print(\"No stratum column detected - will evaluate overall only.\")\n",
        "\n",
        "# ----------------- Data cleaning -----------------\n",
        "df_orig = df_orig.dropna(subset=[y_true_col, y_prob_col]).copy()\n",
        "df_orig['y_true'] = df_orig[y_true_col].astype(int)\n",
        "df_orig['y_prob'] = pd.to_numeric(df_orig[y_prob_col], errors='coerce').clip(0,1)\n",
        "df_orig = df_orig.dropna(subset=['y_prob']).reset_index(drop=True)\n",
        "n_total = len(df_orig)\n",
        "pos_rate_total = df_orig['y_true'].mean()\n",
        "\n",
        "# ----------------- Multi-run evaluation -----------------\n",
        "rng = np.random.default_rng(BASE_SEED)\n",
        "seeds = [int(r) for r in rng.integers(0, 2**31-1, size=K_RUNS)]\n",
        "print(f\"Running {K_RUNS} runs with seeds: {seeds}\")\n",
        "\n",
        "metrics_rows = []\n",
        "topk_rows = []\n",
        "\n",
        "roc_tpr_matrix = np.zeros((K_RUNS, len(ROC_FPR_GRID)))\n",
        "pr_prec_matrix = np.zeros((K_RUNS, len(PR_RECALL_GRID)))\n",
        "cal_bin_obs_matrix = np.zeros((K_RUNS, len(CAL_PROB_BINS)-1))\n",
        "topk_matrix = np.zeros((K_RUNS, len(TOPK_PERCENTS)))\n",
        "\n",
        "for run_idx, seed in enumerate(seeds):\n",
        "    np.random.seed(seed)\n",
        "    # ----------------- Sampling -----------------\n",
        "    if SAMPLING_METHOD == 'stratified_bootstrap':\n",
        "        idx_list = []\n",
        "        for cls in df_orig['y_true'].unique():\n",
        "            cls_idx = df_orig.index[df_orig['y_true']==cls].tolist()\n",
        "            if len(cls_idx)==0:\n",
        "                continue\n",
        "            s = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
        "            idx_list.extend(s.tolist())\n",
        "        sampled_idx = np.array(idx_list)\n",
        "    elif SAMPLING_METHOD == 'subsample_no_replacement':\n",
        "        n_take = int(math.ceil(n_total * TEST_SAMPLE_FRACTION))\n",
        "        sampled_idx = np.random.choice(df_orig.index, size=n_take, replace=False)\n",
        "    df = df_orig.loc[sampled_idx].reset_index(drop=True)\n",
        "\n",
        "    # ----------------- Metrics -----------------\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(df['y_true'], df['y_prob'])\n",
        "    except Exception: roc_auc = np.nan\n",
        "    try:\n",
        "        pr_auc = average_precision_score(df['y_true'], df['y_prob'])\n",
        "    except Exception: pr_auc = np.nan\n",
        "    brier = brier_score_loss(df['y_true'], df['y_prob'])\n",
        "    thresh = 0.5\n",
        "    y_pred = (df['y_prob'] >= thresh).astype(int)\n",
        "    f1 = f1_score(df['y_true'], y_pred, zero_division=0)\n",
        "    prec = precision_score(df['y_true'], y_pred, zero_division=0)\n",
        "    rec = recall_score(df['y_true'], y_pred, zero_division=0)\n",
        "    metrics_rows.append({\n",
        "        \"run\": run_idx, \"seed\": seed, \"n_samples\": len(df),\n",
        "        \"positive_rate\": float(df['y_true'].mean()),\n",
        "        \"roc_auc\": float(roc_auc), \"pr_auc\": float(pr_auc),\n",
        "        \"brier\": float(brier), \"threshold\": thresh,\n",
        "        \"f1_at_0.5\": float(f1),\n",
        "        \"precision_at_0.5\": float(prec),\n",
        "        \"recall_at_0.5\": float(rec)\n",
        "    })\n",
        "\n",
        "    # ----------------- Curves -----------------\n",
        "    try: fpr, tpr, _ = roc_curve(df['y_true'], df['y_prob']); tpr_interp = interpolate_curve(fpr, tpr, ROC_FPR_GRID)\n",
        "    except Exception: tpr_interp = np.full_like(ROC_FPR_GRID, np.nan)\n",
        "    roc_tpr_matrix[run_idx,:] = tpr_interp\n",
        "\n",
        "    try: precision, recall, _ = precision_recall_curve(df['y_true'], df['y_prob'])\n",
        "    except Exception: precision, recall = np.array([]), np.array([])\n",
        "    if len(recall)>1:\n",
        "        recall_sort_idx = np.argsort(recall)\n",
        "        prec_interp = interpolate_curve(recall[recall_sort_idx], precision[recall_sort_idx], PR_RECALL_GRID)\n",
        "    else:\n",
        "        prec_interp = np.full_like(PR_RECALL_GRID, np.nan)\n",
        "    pr_prec_matrix[run_idx,:] = prec_interp\n",
        "\n",
        "    try:\n",
        "        df['prob_bin'] = pd.cut(df['y_prob'], bins=CAL_PROB_BINS, include_lowest=True, labels=False)\n",
        "        obs_per_bin = [df.loc[df['prob_bin']==b, 'y_true'].mean() if (df['prob_bin']==b).sum()>0 else np.nan\n",
        "                       for b in range(len(CAL_PROB_BINS)-1)]\n",
        "        cal_bin_obs_matrix[run_idx,:] = np.array(obs_per_bin, dtype=float)\n",
        "    except Exception:\n",
        "        cal_bin_obs_matrix[run_idx,:] = np.full(len(CAL_PROB_BINS)-1, np.nan)\n",
        "\n",
        "    # ----------------- Top-K -----------------\n",
        "    try:\n",
        "        topk_df = topk_capture(df['y_true'].values, df['y_prob'].values, ks=TOPK_PERCENTS)\n",
        "        for j,k in enumerate(TOPK_PERCENTS):\n",
        "            topk_matrix[run_idx,j] = topk_df.loc[topk_df['top_%']==k, 'pos_captured_frac'].values[0]\n",
        "        temp = topk_df.copy(); temp['run']=run_idx; temp['seed']=seed; topk_rows.append(temp)\n",
        "    except Exception:\n",
        "        topk_matrix[run_idx,:] = np.nan\n",
        "        topk_rows.append(pd.DataFrame())\n",
        "\n",
        "# ----------------- Aggregation -----------------\n",
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "metrics_df.to_csv(METRICS_PER_RUN_CSV, index=False)\n",
        "\n",
        "def agg_stats(series):\n",
        "    mean = np.nanmean(series)\n",
        "    sd = np.nanstd(series, ddof=1) if np.sum(~np.isnan(series))>1 else np.nan\n",
        "    n = np.sum(~np.isnan(series))\n",
        "    se = sd / math.sqrt(n) if n>0 and not np.isnan(sd) else np.nan\n",
        "    ci95 = 1.96 * se if se is not None else np.nan\n",
        "    return mean, sd, ci95\n",
        "\n",
        "agg_rows = []\n",
        "metrics_to_agg = [\"roc_auc\",\"pr_auc\",\"brier\",\"f1_at_0.5\",\"precision_at_0.5\",\"recall_at_0.5\",\"positive_rate\"]\n",
        "for metric in metrics_to_agg:\n",
        "    mean, sd, ci95 = agg_stats(metrics_df[metric].values)\n",
        "    agg_rows.append({\"stratum\": \"ALL\", \"metric\": metric, \"mean\": mean, \"sd\": sd, \"ci95\": ci95})\n",
        "\n",
        "if stratum_col:\n",
        "    strata = df_orig[stratum_col].dropna().unique().tolist()\n",
        "    for stratum_val in strata:\n",
        "        per_stratum_metrics = []\n",
        "        for run_idx, seed in enumerate(seeds):\n",
        "            np.random.seed(seed)\n",
        "            if SAMPLING_METHOD == 'stratified_bootstrap':\n",
        "                idx_list = []\n",
        "                for cls in df_orig['y_true'].unique():\n",
        "                    cls_idx = df_orig.index[df_orig['y_true']==cls].tolist()\n",
        "                    if len(cls_idx)==0: continue\n",
        "                    s = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
        "                    idx_list.extend(s.tolist())\n",
        "                sampled_idx = np.array(idx_list)\n",
        "            else:\n",
        "                n_take = int(math.ceil(n_total * TEST_SAMPLE_FRACTION))\n",
        "                sampled_idx = np.random.choice(df_orig.index, size=n_take, replace=False)\n",
        "            df_run_stratum = df_orig.loc[sampled_idx][df_orig[stratum_col]==stratum_val]\n",
        "            if len(df_run_stratum)==0:\n",
        "                per_stratum_metrics.append({m: np.nan for m in metrics_to_agg})\n",
        "                continue\n",
        "            try: roc_auc = roc_auc_score(df_run_stratum['y_true'], df_run_stratum['y_prob'])\n",
        "            except Exception: roc_auc = np.nan\n",
        "            try: pr_auc = average_precision_score(df_run_stratum['y_true'], df_run_stratum['y_prob'])\n",
        "            except Exception: pr_auc = np.nan\n",
        "            brier = brier_score_loss(df_run_stratum['y_true'], df_run_stratum['y_prob'])\n",
        "            y_pred = (df_run_stratum['y_prob']>=0.5).astype(int)\n",
        "            per_stratum_metrics.append({\n",
        "                \"roc_auc\": roc_auc, \"pr_auc\": pr_auc, \"brier\": brier,\n",
        "                \"f1_at_0.5\": f1_score(df_run_stratum['y_true'], y_pred, zero_division=0),\n",
        "                \"precision_at_0.5\": precision_score(df_run_stratum['y_true'], y_pred, zero_division=0),\n",
        "                \"recall_at_0.5\": recall_score(df_run_stratum['y_true'], y_pred, zero_division=0),\n",
        "                \"positive_rate\": float(df_run_stratum['y_true'].mean())\n",
        "            })\n",
        "        per_stratum_df = pd.DataFrame(per_stratum_metrics)\n",
        "        for metric in metrics_to_agg:\n",
        "            mean, sd, ci95 = agg_stats(per_stratum_df[metric].values)\n",
        "            agg_rows.append({\"stratum\": str(stratum_val), \"metric\": metric, \"mean\": mean, \"sd\": sd, \"ci95\": ci95})\n",
        "\n",
        "agg_df = pd.DataFrame(agg_rows)\n",
        "agg_df.to_csv(METRICS_AGG_CSV, index=False)\n",
        "\n",
        "# ----------------- Top-K aggregation -----------------\n",
        "topk_all = pd.concat([t.assign(run=int(t['run'].iloc[0])) if not t.empty else pd.DataFrame() for t in topk_rows], ignore_index=True, sort=False)\n",
        "if not topk_all.empty: topk_all.to_csv(TOPK_PER_RUN_CSV, index=False)\n",
        "topk_agg = [{\"top_%\": k, \"mean_frac\": agg_stats(topk_matrix[:,j])[0],\n",
        "             \"sd\": agg_stats(topk_matrix[:,j])[1], \"ci95\": agg_stats(topk_matrix[:,j])[2]}\n",
        "            for j,k in enumerate(TOPK_PERCENTS)]\n",
        "pd.DataFrame(topk_agg).to_csv(TOPK_AGG_CSV, index=False)\n",
        "\n",
        "# ----------------- Plots (ROC, PR, Top-K, Reliability) -----------------\n",
        "# Convert to arrays\n",
        "roc_arr = np.array(roc_list)\n",
        "pr_arr = np.array(pr_list)\n",
        "cal_arr = np.array(cal_list)\n",
        "topk_arr = np.array(topk_list)\n",
        "metrics_df = pd.DataFrame(metrics_all)\n",
        "\n",
        "# ----------------- Plotting with ribbons -----------------\n",
        "ROC_FPR_GRID = np.linspace(0,1,100)\n",
        "PR_RECALL_GRID = np.linspace(0,1,100)\n",
        "TOPK_PERCENTS = np.linspace(0.01,1,100)\n",
        "CAL_PROB_BINS = np.linspace(0,1,11)  # 10 bins\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ---------- ROC Curve ----------\n",
        "mean_tpr = np.nanmean(roc_arr, axis=0)\n",
        "sd_tpr = np.nanstd(roc_arr, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(ROC_FPR_GRID, mean_tpr, label=f\"Mean ROC (n={NUM_RUNS})\", linewidth=2)\n",
        "plt.fill_between(ROC_FPR_GRID, mean_tpr - sd_tpr, mean_tpr + sd_tpr, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', color='gray', label='Random')\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=18)\n",
        "plt.ylabel('True Positive Rate', fontsize=18)\n",
        "plt.title('ROC Curve (mean Â± SD)', fontsize=18)\n",
        "plt.legend(loc='lower right', fontsize=18)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'roc_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ---------- PR Curve ----------\n",
        "mean_prec = np.nanmean(pr_arr, axis=0)\n",
        "sd_prec = np.nanstd(pr_arr, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(PR_RECALL_GRID, mean_prec, label=f\"Mean PR (n={NUM_RUNS})\", linewidth=2)\n",
        "plt.fill_between(PR_RECALL_GRID, mean_prec - sd_prec, mean_prec + sd_prec, alpha=0.2)\n",
        "\n",
        "plt.xlabel('Recall', fontsize=18)\n",
        "plt.ylabel('Precision', fontsize=18)\n",
        "plt.title('Precision-Recall Curve (mean Â± SD)', fontsize=18)\n",
        "plt.legend(loc='upper right', fontsize=18)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'pr_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ---------- Top-K Capture Plot ----------\n",
        "n_topk = topk_matrix.shape[1]\n",
        "x_vals = TOPK_PERCENTS if 'TOPK_PERCENTS' in globals() and len(TOPK_PERCENTS)==n_topk else np.linspace(0, 50, n_topk)\n",
        "mean_topk = np.nanmean(topk_matrix, axis=0)\n",
        "sd_topk = np.nanstd(topk_matrix, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x_vals, mean_topk, marker='o', color='tab:blue', linewidth=2, label='Mean Top-K capture')\n",
        "plt.fill_between(x_vals, mean_topk - sd_topk, mean_topk + sd_topk, color='tab:blue', alpha=0.2, label='Â± SD')\n",
        "\n",
        "plt.xlabel('Top-k percent of highest risk area', fontsize=18)\n",
        "plt.ylabel('Fraction of fires captured', fontsize=18)\n",
        "plt.title('Top-K Capture (mean Â± SD)', fontsize=18)\n",
        "plt.ylim(0,1)\n",
        "plt.xlim(0,50)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend(loc='lower right', fontsize=18)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'topk_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ---------- Reliability / Calibration ----------\n",
        "cal_bin_centers = (CAL_PROB_BINS[:-1] + CAL_PROB_BINS[1:]) / 2.0\n",
        "mean_bin_obs = np.nanmean(cal_arr, axis=0)\n",
        "sd_bin_obs = np.nanstd(cal_arr, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(cal_bin_centers, mean_bin_obs, marker='o', linewidth=2, label='Mean calibration')\n",
        "plt.fill_between(cal_bin_centers, mean_bin_obs - sd_bin_obs, mean_bin_obs + sd_bin_obs, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', color='gray', label='Perfect')\n",
        "\n",
        "plt.xlabel('Predicted probability (bin center)', fontsize=16)\n",
        "plt.ylabel('Observed frequency', fontsize=16)\n",
        "plt.title('Reliability Diagram (mean Â± SD)', fontsize=16)\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'reliability_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ----------------- Summary Text -----------------\n",
        "report_lines = [\n",
        "    \"Region-transfer / Temporal-style Multi-run Evaluation\",\n",
        "    f\"File: {csv_path}\",\n",
        "    f\"Total original samples: {n_total}, Overall positive rate: {pos_rate_total:.4f}\",\n",
        "    f\"Sampling method: {SAMPLING_METHOD}\",\n",
        "    f\"Number of runs (K): {K_RUNS}\",\n",
        "    f\"Base seed (document for reproducibility): {BASE_SEED}\",\n",
        "    f\"Seeds used: {seeds}\",\n",
        "    \"\",\n",
        "    f\"Metrics per-run saved to: {METRICS_PER_RUN_CSV}\",\n",
        "    f\"Aggregated metrics saved to: {METRICS_AGG_CSV} (mean Â± sd Â± 95%CI)\",\n",
        "    f\"Top-K per-run saved to: {TOPK_PER_RUN_CSV}\",\n",
        "    f\"Top-K aggregated saved to: {TOPK_AGG_CSV}\",\n",
        "    f\"ROC plot (mean Â± SD): {OUT_DIR / 'roc_curve_mean_sd.png'}\",\n",
        "    f\"PR plot (mean Â± SD): {OUT_DIR / 'pr_curve_mean_sd.png'}\",\n",
        "    f\"Reliability plot (mean Â± SD): {OUT_DIR / 'reliability_mean_sd.png'}\",\n",
        "    f\"Top-K plot (mean Â± SD): {OUT_DIR / 'topk_curve_mean_sd.png'}\",\n",
        "]\n",
        "with open(SUMMARY_TXT, 'w') as f:\n",
        "    f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "print(\"\\n\".join(report_lines))\n",
        "print(\"Outputs written to:\", OUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}