{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jintubhuyan-2000/ML-XAI_ForestFire/blob/main/Forest_RegionTransfer_California_FinalRevised2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown --quiet\n",
        "\n",
        "# Use gdown to download the folder\n",
        "!gdown \"https://drive.google.com/drive/u/0/folders/17bKKw2k12s_ZxHLfAHPhyyxghZXI-mFZ\" --folder"
      ],
      "metadata": {
        "id": "2bOUvWn2HTXI",
        "outputId": "fc6f0596-29dc-4332-f8e3-be281fd2be33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2bOUvWn2HTXI",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1XxhtG6cFx9kNEaBkzDg9W6lyHw1nQnvE AccuracyStats_2025.csv\n",
            "Processing file 1z6J3c-emQt0Skm9yu67-M16358O8XaKq ConfusionMatrix_2025.csv\n",
            "Processing file 1DH1klTmk-Ev4ZMXDvUM8cu6mfGoe-62F FireProbStats_PerDistrict_forest.csv\n",
            "Processing file 1eLfv4KjNlv8KV9ojecww39U8Y_umCfHE RF_RegionTransfer_Test_forest.csv\n",
            "Processing file 1Zc4MgZfGD6hprmwRMcOWz0Y-rEs6ma9Y RF_SpatialCV_Test_forest.csv\n",
            "Processing file 1KC-6mjAcWXP5ycuRb4Bpv6fqtELZEzML RF_TemporalSplit_Test2025.csv\n",
            "Processing file 1bj7yXyCzvKMrlIwpyEl3ou01n-26BA1N TestPoints_Predictors_forest.csv\n",
            "Processing file 1kf6Nome2XrQD3edOhNWuDkF4Vyy5nyGw TrainPoints_Predictors_forest.csv\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XxhtG6cFx9kNEaBkzDg9W6lyHw1nQnvE\n",
            "To: /content/Validation Datasets/AccuracyStats_2025.csv\n",
            "100% 126/126 [00:00<00:00, 494kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z6J3c-emQt0Skm9yu67-M16358O8XaKq\n",
            "To: /content/Validation Datasets/ConfusionMatrix_2025.csv\n",
            "100% 136/136 [00:00<00:00, 609kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DH1klTmk-Ev4ZMXDvUM8cu6mfGoe-62F\n",
            "To: /content/Validation Datasets/FireProbStats_PerDistrict_forest.csv\n",
            "100% 3.62M/3.62M [00:00<00:00, 98.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eLfv4KjNlv8KV9ojecww39U8Y_umCfHE\n",
            "To: /content/Validation Datasets/RF_RegionTransfer_Test_forest.csv\n",
            "100% 328k/328k [00:00<00:00, 125MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zc4MgZfGD6hprmwRMcOWz0Y-rEs6ma9Y\n",
            "To: /content/Validation Datasets/RF_SpatialCV_Test_forest.csv\n",
            "100% 328k/328k [00:00<00:00, 104MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KC-6mjAcWXP5ycuRb4Bpv6fqtELZEzML\n",
            "To: /content/Validation Datasets/RF_TemporalSplit_Test2025.csv\n",
            "100% 411k/411k [00:00<00:00, 72.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bj7yXyCzvKMrlIwpyEl3ou01n-26BA1N\n",
            "To: /content/Validation Datasets/TestPoints_Predictors_forest.csv\n",
            "100% 351k/351k [00:00<00:00, 138MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kf6Nome2XrQD3edOhNWuDkF4Vyy5nyGw\n",
            "To: /content/Validation Datasets/TrainPoints_Predictors_forest.csv\n",
            "100% 71.5k/71.5k [00:00<00:00, 76.6MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KX8PgDM6HUtO"
      },
      "id": "KX8PgDM6HUtO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8e889d0a-8210-4ca6-8dd5-50862ae85fb0",
      "metadata": {
        "id": "8e889d0a-8210-4ca6-8dd5-50862ae85fb0",
        "outputId": "f16530a2-cdd1-43c0-ee87-e1ec4fca9736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train CSV: /content/Validation Datasets/TrainPoints_Predictors_forest.csv\n",
            "Test CSV: /content/Validation Datasets/RF_TemporalSplit_Test2025.csv\n",
            "Saved per-run metrics: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/metrics_per_run.csv\n",
            "Saved per-run Top-K captures: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/topk_per_run.csv\n",
            "No 'stratum' column found in test CSV; skipping aggregation by stratum.\n",
            "Summary report written: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/summary_report_multi_run.txt\n",
            "\n",
            "All done. Check results in: /content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Wildfire RF analysis with multiple runs & uncertainty.\n",
        "Generates mean ± SD curves for ROC, PR, Calibration, Top-K.\n",
        "Saves per-run metrics, Top-K captures, aggregated metrics by stratum, and summary.\n",
        "\"\"\"\n",
        "\n",
        "import os, glob, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
        "                             roc_curve, precision_recall_curve, f1_score, precision_score, recall_score)\n",
        "from sklearn.calibration import calibration_curve\n",
        "import joblib\n",
        "\n",
        "# Optional SHAP\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "    warnings.warn('shap not available. Install shap to compute SHAP plots.')\n",
        "\n",
        "# ----------------------\n",
        "# Paths\n",
        "# ----------------------\n",
        "BASE_DIR = r\"/content/Validation Datasets\"\n",
        "RESULTS_DIR = os.path.join('/content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5')\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "OUT_DIR = RESULTS_DIR\n",
        "METRICS_PER_RUN_CSV = os.path.join(OUT_DIR, \"metrics_per_run.csv\")\n",
        "METRICS_AGG_CSV = os.path.join(OUT_DIR, \"metrics_aggregated_by_stratum.csv\")\n",
        "TOPK_PER_RUN_CSV = os.path.join(OUT_DIR, \"topk_per_run.csv\")\n",
        "TOPK_AGG_CSV = os.path.join(OUT_DIR, \"topk_aggregated.csv\")\n",
        "SUMMARY_TXT = os.path.join(OUT_DIR, \"summary_report_multi_run.txt\")\n",
        "\n",
        "# ----------------------\n",
        "# CSV detection\n",
        "# ----------------------\n",
        "csv_files = glob.glob(os.path.join(BASE_DIR, '**', '*.csv'), recursive=True)\n",
        "def find_candidate(files, keywords):\n",
        "    keywords = [k.lower() for k in keywords]\n",
        "    for f in files:\n",
        "        name = os.path.basename(f).lower()\n",
        "        if any(k in name for k in keywords):\n",
        "            return f\n",
        "    return None\n",
        "\n",
        "train_csv = find_candidate(csv_files, ['train'])\n",
        "test_csv = find_candidate(csv_files, ['test'])\n",
        "if not train_csv or not test_csv:\n",
        "    csv_sizes = sorted(csv_files, key=os.path.getsize, reverse=True)\n",
        "    if len(csv_sizes) >= 2:\n",
        "        train_csv = csv_sizes[0] if not train_csv else train_csv\n",
        "        test_csv = csv_sizes[1] if not test_csv else test_csv\n",
        "\n",
        "print('Train CSV:', train_csv)\n",
        "print('Test CSV:', test_csv)\n",
        "\n",
        "train = pd.read_csv(train_csv)\n",
        "test = pd.read_csv(test_csv)\n",
        "\n",
        "# Clean column names\n",
        "train.columns = [c.strip() for c in train.columns]\n",
        "test.columns = [c.strip() for c in test.columns]\n",
        "\n",
        "# Target column\n",
        "possible_targets = ['class', 'y', 'label', 'fire', 'is_fire']\n",
        "train_cols_low = [c.lower() for c in train.columns]\n",
        "target_col = next((train.columns[i] for i,t in enumerate(train_cols_low) if t in possible_targets), None)\n",
        "if target_col is None:\n",
        "    raise ValueError(\"Target column not found\")\n",
        "\n",
        "train[target_col] = train[target_col].astype(int)\n",
        "if target_col in test.columns:\n",
        "    test[target_col] = test[target_col].astype(int)\n",
        "\n",
        "y_train = train[target_col].values\n",
        "y_true = test[target_col].values if target_col in test.columns else None\n",
        "\n",
        "# Predictor columns\n",
        "predictors = [c for c in train.select_dtypes(include=[np.number]).columns\n",
        "              if c != target_col and 'id' not in c.lower()]\n",
        "X_train = train[predictors].fillna(-999)\n",
        "X_test = test[predictors].fillna(-999)\n",
        "\n",
        "# ----------------------\n",
        "# Multiple runs\n",
        "# ----------------------\n",
        "NUM_RUNS = 10\n",
        "SEEDS = list(range(42, 42+NUM_RUNS))\n",
        "\n",
        "# Storage\n",
        "roc_list, pr_list, cal_list, topk_list = [], [], [], []\n",
        "metrics_all = []\n",
        "y_prob_runs = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=seed, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    joblib.dump(rf, os.path.join(RESULTS_DIR, f'rf_model_{seed}.joblib'))\n",
        "\n",
        "    y_prob = rf.predict_proba(X_test)[:,1]\n",
        "    y_prob_runs.append(y_prob)\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'seed': seed,\n",
        "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
        "        'pr_auc': average_precision_score(y_true, y_prob),\n",
        "        'brier': brier_score_loss(y_true, y_prob),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred)\n",
        "    }\n",
        "    metrics_all.append(metrics)\n",
        "\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_list.append(np.interp(np.linspace(0,1,100), fpr, tpr))\n",
        "\n",
        "    # PR\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "    pr_list.append(np.interp(np.linspace(0,1,100), recall[::-1], precision[::-1]))\n",
        "\n",
        "    # Calibration\n",
        "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
        "    cal_list.append(np.interp(np.linspace(0,1,10), prob_pred, prob_true))\n",
        "\n",
        "    # Top-K capture\n",
        "    df_test = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob}).sort_values('y_prob', ascending=False)\n",
        "    total_pos = df_test['y_true'].sum()\n",
        "    topk_capture = [df_test.iloc[:int(np.ceil(len(df_test)*f))]['y_true'].sum()/total_pos\n",
        "                    for f in np.linspace(0.01,1,100)]\n",
        "    topk_list.append(topk_capture)\n",
        "\n",
        "# Convert to arrays\n",
        "roc_arr = np.array(roc_list)\n",
        "pr_arr = np.array(pr_list)\n",
        "cal_arr = np.array(cal_list)\n",
        "topk_arr = np.array(topk_list)\n",
        "metrics_df = pd.DataFrame(metrics_all)\n",
        "\n",
        "# ----------------- Plotting with ribbons -----------------\n",
        "ROC_FPR_GRID = np.linspace(0,1,100)\n",
        "PR_RECALL_GRID = np.linspace(0,1,100)\n",
        "TOPK_PERCENTS = np.linspace(0.01,1,100)\n",
        "CAL_PROB_BINS = np.linspace(0,1,11)  # 10 bins\n",
        "\n",
        "# ---------- ROC Curve ----------\n",
        "mean_tpr = np.nanmean(roc_arr, axis=0)\n",
        "sd_tpr = np.nanstd(roc_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(ROC_FPR_GRID, mean_tpr, label=f\"Mean ROC (n={NUM_RUNS})\")\n",
        "plt.fill_between(ROC_FPR_GRID, mean_tpr - sd_tpr, mean_tpr + sd_tpr, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve (mean ± SD)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'roc_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ---------- PR Curve ----------\n",
        "mean_prec = np.nanmean(pr_arr, axis=0)\n",
        "sd_prec = np.nanstd(pr_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(PR_RECALL_GRID, mean_prec, label=f\"Mean PR (n={NUM_RUNS})\")\n",
        "plt.fill_between(PR_RECALL_GRID, mean_prec - sd_prec, mean_prec + sd_prec, alpha=0.2)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (mean ± SD)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'pr_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ---------- Top-K Capture ----------\n",
        "mean_topk = np.nanmean(topk_arr, axis=0)\n",
        "sd_topk = np.nanstd(topk_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(TOPK_PERCENTS, mean_topk, marker='o', label='Mean Top-K capture')\n",
        "plt.fill_between(TOPK_PERCENTS, mean_topk - sd_topk, mean_topk + sd_topk, alpha=0.2)\n",
        "plt.xlabel('Top-k percent of highest risk area')\n",
        "plt.ylabel('Fraction of fires captured')\n",
        "plt.title('Top-K Capture (mean ± SD)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'topk_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ---------- Reliability / Calibration ----------\n",
        "cal_bin_centers = (CAL_PROB_BINS[:-1] + CAL_PROB_BINS[1:]) / 2.0\n",
        "mean_bin_obs = np.nanmean(cal_arr, axis=0)\n",
        "sd_bin_obs = np.nanstd(cal_arr, axis=0, ddof=1)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(cal_bin_centers, mean_bin_obs, marker='o', label='Mean calibration')\n",
        "plt.fill_between(cal_bin_centers, mean_bin_obs - sd_bin_obs, mean_bin_obs + sd_bin_obs, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', label='Perfect')\n",
        "plt.xlabel('Predicted probability (bin center)')\n",
        "plt.ylabel('Observed frequency')\n",
        "plt.title('Reliability Diagram (mean ± SD)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'reliability_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# ----------------- Save per-run metrics -----------------\n",
        "metrics_df.to_csv(METRICS_PER_RUN_CSV, index=False)\n",
        "topk_df = pd.DataFrame(topk_arr, columns=TOPK_PERCENTS)\n",
        "topk_df['seed'] = SEEDS\n",
        "topk_df.to_csv(TOPK_PER_RUN_CSV, index=False)\n",
        "print(f\"Saved per-run metrics: {METRICS_PER_RUN_CSV}\")\n",
        "print(f\"Saved per-run Top-K captures: {TOPK_PER_RUN_CSV}\")\n",
        "\n",
        "# ----------------- Aggregate metrics by stratum -----------------\n",
        "if 'stratum' in test.columns:\n",
        "    stratum_vals = test['stratum'].unique()\n",
        "    agg_metrics_list = []\n",
        "    agg_topk_list = []\n",
        "\n",
        "    for s in stratum_vals:\n",
        "        idx = test[test['stratum']==s].index\n",
        "        metrics_sub, topk_sub = [], []\n",
        "\n",
        "        for seed_i, seed in enumerate(SEEDS):\n",
        "            y_prob_sub = y_prob_runs[seed_i][idx]\n",
        "            y_true_sub = y_true[idx]\n",
        "            y_pred_sub = (y_prob_sub >= 0.5).astype(int)\n",
        "\n",
        "            metrics_sub.append({\n",
        "                'seed': seed,\n",
        "                'stratum': s,\n",
        "                'roc_auc': roc_auc_score(y_true_sub, y_prob_sub),\n",
        "                'pr_auc': average_precision_score(y_true_sub, y_prob_sub),\n",
        "                'brier': brier_score_loss(y_true_sub, y_prob_sub),\n",
        "                'f1': f1_score(y_true_sub, y_pred_sub),\n",
        "                'precision': precision_score(y_true_sub, y_pred_sub),\n",
        "                'recall': recall_score(y_true_sub, y_pred_sub)\n",
        "            })\n",
        "\n",
        "            df_sub = pd.DataFrame({'y_true': y_true_sub, 'y_prob': y_prob_sub}).sort_values('y_prob', ascending=False)\n",
        "            total_pos_sub = df_sub['y_true'].sum()\n",
        "            topk_capture_sub = [df_sub.iloc[:int(np.ceil(len(df_sub)*f))]['y_true'].sum()/total_pos_sub\n",
        "                                for f in TOPK_PERCENTS]\n",
        "            topk_sub.append(topk_capture_sub)\n",
        "\n",
        "        metrics_sub_df = pd.DataFrame(metrics_sub)\n",
        "        agg_metrics_list.append(metrics_sub_df.groupby('stratum').agg(['mean','std']))\n",
        "\n",
        "        topk_sub_arr = np.array(topk_sub)\n",
        "        topk_mean = np.nanmean(topk_sub_arr, axis=0)\n",
        "        topk_sd = np.nanstd(topk_sub_arr, axis=0, ddof=1)\n",
        "        agg_topk_list.append(pd.DataFrame({'stratum': s, 'topk_percent': TOPK_PERCENTS,\n",
        "                                          'mean_capture': topk_mean, 'sd_capture': topk_sd}))\n",
        "\n",
        "    pd.concat(agg_metrics_list).to_csv(METRICS_AGG_CSV)\n",
        "    pd.concat(agg_topk_list).to_csv(TOPK_AGG_CSV, index=False)\n",
        "    print(f\"Saved aggregated metrics by stratum: {METRICS_AGG_CSV}\")\n",
        "    print(f\"Saved aggregated Top-K by stratum: {TOPK_AGG_CSV}\")\n",
        "else:\n",
        "    print(\"No 'stratum' column found in test CSV; skipping aggregation by stratum.\")\n",
        "\n",
        "# ----------------- Summary -----------------\n",
        "with open(SUMMARY_TXT,'w') as fh:\n",
        "    fh.write(f'Random Forest multiple runs evaluation ({NUM_RUNS} runs)\\n')\n",
        "    fh.write('===============================\\n')\n",
        "    fh.write(f'Train CSV: {train_csv}\\n')\n",
        "    fh.write(f'Test CSV: {test_csv}\\n')\n",
        "    fh.write(f'Seed list: {SEEDS}\\n\\n')\n",
        "    fh.write('Metrics (mean ± SD):\\n')\n",
        "    mean_metrics = metrics_df.mean()\n",
        "    sd_metrics = metrics_df.std()\n",
        "    for col in metrics_df.columns:\n",
        "        if col != 'seed':\n",
        "            fh.write(f' - {col}: {mean_metrics[col]:.4f} ± {sd_metrics[col]:.4f}\\n')\n",
        "    fh.write('\\nAll plots saved in the results directory.\\n')\n",
        "print(f\"Summary report written: {SUMMARY_TXT}\")\n",
        "\n",
        "print('\\nAll done. Check results in:', RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "67dedc0a-3b02-4102-97cc-748a046dc888",
      "metadata": {
        "id": "67dedc0a-3b02-4102-97cc-748a046dc888",
        "outputId": "d0dc2fce-ab8c-418f-ea81-284f32e91eb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CSV: /content/Validation Datasets/RF_TemporalSplit_Test2025.csv\n",
            "Detected columns -> label: class , prob: classification\n",
            "No stratum column detected - will evaluate overall only.\n",
            "Running 10 runs with seeds: [1281540326, 1005233768, 2011547310, 1367058049, 1542280147, 90017157, 581114276, 1258272007, 2134214070, 1923482161]\n",
            "Region-transfer / Temporal-style Multi-run Evaluation\n",
            "File: /content/Validation Datasets/RF_TemporalSplit_Test2025.csv\n",
            "Total original samples: 2148, Overall positive rate: 0.7658\n",
            "Sampling method: stratified_bootstrap\n",
            "Number of runs (K): 10\n",
            "Base seed (document for reproducibility): 20250908\n",
            "Seeds used: [1281540326, 1005233768, 2011547310, 1367058049, 1542280147, 90017157, 581114276, 1258272007, 2134214070, 1923482161]\n",
            "\n",
            "Metrics per-run saved to: //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/metrics_per_run.csv\n",
            "Aggregated metrics saved to: //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/metrics_aggregated_by_stratum.csv (mean ± sd ± 95%CI)\n",
            "Top-K per-run saved to: //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/topk_per_run.csv\n",
            "Top-K aggregated saved to: //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/topk_aggregated.csv\n",
            "ROC plot (mean ± SD): //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/roc_curve_mean_sd.png\n",
            "PR plot (mean ± SD): //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/pr_curve_mean_sd.png\n",
            "Reliability plot (mean ± SD): //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/reliability_mean_sd.png\n",
            "Top-K plot (mean ± SD): //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5/topk_curve_mean_sd.png\n",
            "Outputs written to: //content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5\n"
          ]
        }
      ],
      "source": [
        "# Wildfire RF Evaluation & SHAP Analysis (multi-run + uncertainty)\n",
        "# Paste into a Jupyter cell. Requires: pandas, numpy, matplotlib, scikit-learn\n",
        "# Optional: shap (pip install shap) for SHAP outputs.\n",
        "# Outputs: metrics CSV (per-run + aggregated), topk CSV, curves, summary text.\n",
        "# Author: adapted for multi-run uncertainty & reproducibility\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, roc_curve,\n",
        "    average_precision_score, precision_recall_curve,\n",
        "    brier_score_loss, f1_score, precision_score, recall_score\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# Optional SHAP\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "search_dirs = [\n",
        "    r\"/content/Validation Datasets\",\n",
        "]\n",
        "OUT_DIR = Path(r\"//content/drive/MyDrive/California_Fire_MS/Fire_Risk_Validation/forest/Results/results_region_transfer_V5\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# resampling / reproducibility settings\n",
        "K_RUNS = 10\n",
        "BASE_SEED = 20250908\n",
        "SAMPLING_METHOD = \"stratified_bootstrap\"\n",
        "TEST_SAMPLE_FRACTION = 1.0\n",
        "\n",
        "# Settings for curves / interpolation\n",
        "ROC_FPR_GRID = np.linspace(0,1,200)\n",
        "PR_RECALL_GRID = np.linspace(0,1,200)\n",
        "CAL_PROB_BINS = np.linspace(0.0, 1.0, 11)\n",
        "TOPK_PERCENTS = [1,5,10,20,30,40,50]\n",
        "\n",
        "# Filenames\n",
        "METRICS_PER_RUN_CSV = OUT_DIR / \"metrics_per_run.csv\"\n",
        "METRICS_AGG_CSV = OUT_DIR / \"metrics_aggregated_by_stratum.csv\"\n",
        "TOPK_PER_RUN_CSV = OUT_DIR / \"topk_per_run.csv\"\n",
        "TOPK_AGG_CSV = OUT_DIR / \"topk_aggregated.csv\"\n",
        "SUMMARY_TXT = OUT_DIR / \"summary_report_multi_run.txt\"\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def find_candidate_csv(dirs):\n",
        "    candidates = []\n",
        "    for d in dirs:\n",
        "        for p in Path(d).rglob(\"*.csv\"):\n",
        "            candidates.append(p)\n",
        "    if not candidates:\n",
        "        return None\n",
        "    prioritized = [p for p in candidates if any(k in p.name.lower() for k in (\"test2025\",\"test_2025\",\"rf_regiontransfer\",\"rf_spatialcv_test\",\"rf_region_transfer_test\",\"rf_spatialcv_test\",\"rf_\"))]\n",
        "    if prioritized:\n",
        "        return prioritized[0]\n",
        "    prioritized = [p for p in candidates if any(k in p.name.lower() for k in (\"test\",\"2025\",\"regiontransfer\",\"region_transfer\"))]\n",
        "    if prioritized:\n",
        "        return prioritized[0]\n",
        "    return candidates[0]\n",
        "\n",
        "def topk_capture(y_true, y_prob, ks=TOPK_PERCENTS):\n",
        "    out = []\n",
        "    order = np.argsort(-y_prob)\n",
        "    y_true_sorted = np.array(y_true)[order]\n",
        "    total_pos = float(y_true_sorted.sum())\n",
        "    n = len(y_true_sorted)\n",
        "    for k in ks:\n",
        "        frac = k / 100.0\n",
        "        top_n = max(1, int(math.ceil(n * frac)))\n",
        "        captured = int(y_true_sorted[:top_n].sum())\n",
        "        capture_rate = (captured / total_pos) if total_pos > 0 else np.nan\n",
        "        out.append({\n",
        "            \"top_%\": k,\n",
        "            \"top_n\": top_n,\n",
        "            \"pos_captured_count\": captured,\n",
        "            \"pos_captured_frac\": capture_rate\n",
        "        })\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "def interpolate_curve(x, y, x_grid):\n",
        "    xp = np.clip(x, 0.0, 1.0)\n",
        "    yp = np.clip(y, 0.0, 1.0)\n",
        "    xp_unique, idx = np.unique(xp, return_index=True)\n",
        "    yp_unique = yp[idx]\n",
        "    if len(xp_unique) < 2:\n",
        "        return np.full_like(x_grid, yp_unique[0] if len(yp_unique)>0 else np.nan)\n",
        "    return np.interp(x_grid, xp_unique, yp_unique)\n",
        "\n",
        "# ----------------- Locate CSV -----------------\n",
        "csv_path = find_candidate_csv(search_dirs)\n",
        "if csv_path is None:\n",
        "    raise FileNotFoundError(f\"No CSV found in {search_dirs}. Place your exported CSV(s) in one of those folders.\")\n",
        "print(\"Using CSV:\", csv_path)\n",
        "\n",
        "# ----------------- Load & detect columns -----------------\n",
        "df_orig = pd.read_csv(csv_path)\n",
        "cols = list(df_orig.columns)\n",
        "y_true_col = None\n",
        "y_prob_col = None\n",
        "\n",
        "for c in cols:\n",
        "    lc = c.lower().strip()\n",
        "    if lc in ('class','label','y','ground_truth','is_fire','fire') and y_true_col is None:\n",
        "        y_true_col = c\n",
        "    if lc in ('classification','probability','prob','pred','pred_prob','probability_1') and y_prob_col is None:\n",
        "        y_prob_col = c\n",
        "# fallback heuristics\n",
        "if y_true_col is None:\n",
        "    for c in cols:\n",
        "        if 'class' in c.lower() or c.lower().startswith('label'):\n",
        "            y_true_col = c\n",
        "            break\n",
        "if y_prob_col is None:\n",
        "    for c in cols:\n",
        "        if any(k in c.lower() for k in ('prob','classification','pred')):\n",
        "            y_prob_col = c\n",
        "            break\n",
        "if y_true_col is None or y_prob_col is None:\n",
        "    raise ValueError(f\"Could not auto-detect required columns. Found: {cols}\")\n",
        "print(\"Detected columns -> label:\", y_true_col, \", prob:\", y_prob_col)\n",
        "\n",
        "# optional stratum column detection\n",
        "stratum_col = None\n",
        "for candidate in ['landcover','land_cover','lc','class_name','stratum','habitat','lcc']:\n",
        "    if candidate in [c.lower() for c in cols]:\n",
        "        for c in cols:\n",
        "            if c.lower()==candidate:\n",
        "                stratum_col = c\n",
        "                break\n",
        "        break\n",
        "if stratum_col:\n",
        "    print(\"Detected stratum column:\", stratum_col)\n",
        "else:\n",
        "    print(\"No stratum column detected - will evaluate overall only.\")\n",
        "\n",
        "# ----------------- Data cleaning -----------------\n",
        "df_orig = df_orig.dropna(subset=[y_true_col, y_prob_col]).copy()\n",
        "df_orig['y_true'] = df_orig[y_true_col].astype(int)\n",
        "df_orig['y_prob'] = pd.to_numeric(df_orig[y_prob_col], errors='coerce').clip(0,1)\n",
        "df_orig = df_orig.dropna(subset=['y_prob']).reset_index(drop=True)\n",
        "n_total = len(df_orig)\n",
        "pos_rate_total = df_orig['y_true'].mean()\n",
        "\n",
        "# ----------------- Multi-run evaluation -----------------\n",
        "rng = np.random.default_rng(BASE_SEED)\n",
        "seeds = [int(r) for r in rng.integers(0, 2**31-1, size=K_RUNS)]\n",
        "print(f\"Running {K_RUNS} runs with seeds: {seeds}\")\n",
        "\n",
        "metrics_rows = []\n",
        "topk_rows = []\n",
        "\n",
        "roc_tpr_matrix = np.zeros((K_RUNS, len(ROC_FPR_GRID)))\n",
        "pr_prec_matrix = np.zeros((K_RUNS, len(PR_RECALL_GRID)))\n",
        "cal_bin_obs_matrix = np.zeros((K_RUNS, len(CAL_PROB_BINS)-1))\n",
        "topk_matrix = np.zeros((K_RUNS, len(TOPK_PERCENTS)))\n",
        "\n",
        "for run_idx, seed in enumerate(seeds):\n",
        "    np.random.seed(seed)\n",
        "    # ----------------- Sampling -----------------\n",
        "    if SAMPLING_METHOD == 'stratified_bootstrap':\n",
        "        idx_list = []\n",
        "        for cls in df_orig['y_true'].unique():\n",
        "            cls_idx = df_orig.index[df_orig['y_true']==cls].tolist()\n",
        "            if len(cls_idx)==0:\n",
        "                continue\n",
        "            s = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
        "            idx_list.extend(s.tolist())\n",
        "        sampled_idx = np.array(idx_list)\n",
        "    elif SAMPLING_METHOD == 'subsample_no_replacement':\n",
        "        n_take = int(math.ceil(n_total * TEST_SAMPLE_FRACTION))\n",
        "        sampled_idx = np.random.choice(df_orig.index, size=n_take, replace=False)\n",
        "    df = df_orig.loc[sampled_idx].reset_index(drop=True)\n",
        "\n",
        "    # ----------------- Metrics -----------------\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(df['y_true'], df['y_prob'])\n",
        "    except Exception: roc_auc = np.nan\n",
        "    try:\n",
        "        pr_auc = average_precision_score(df['y_true'], df['y_prob'])\n",
        "    except Exception: pr_auc = np.nan\n",
        "    brier = brier_score_loss(df['y_true'], df['y_prob'])\n",
        "    thresh = 0.5\n",
        "    y_pred = (df['y_prob'] >= thresh).astype(int)\n",
        "    f1 = f1_score(df['y_true'], y_pred, zero_division=0)\n",
        "    prec = precision_score(df['y_true'], y_pred, zero_division=0)\n",
        "    rec = recall_score(df['y_true'], y_pred, zero_division=0)\n",
        "    metrics_rows.append({\n",
        "        \"run\": run_idx, \"seed\": seed, \"n_samples\": len(df),\n",
        "        \"positive_rate\": float(df['y_true'].mean()),\n",
        "        \"roc_auc\": float(roc_auc), \"pr_auc\": float(pr_auc),\n",
        "        \"brier\": float(brier), \"threshold\": thresh,\n",
        "        \"f1_at_0.5\": float(f1),\n",
        "        \"precision_at_0.5\": float(prec),\n",
        "        \"recall_at_0.5\": float(rec)\n",
        "    })\n",
        "\n",
        "    # ----------------- Curves -----------------\n",
        "    try: fpr, tpr, _ = roc_curve(df['y_true'], df['y_prob']); tpr_interp = interpolate_curve(fpr, tpr, ROC_FPR_GRID)\n",
        "    except Exception: tpr_interp = np.full_like(ROC_FPR_GRID, np.nan)\n",
        "    roc_tpr_matrix[run_idx,:] = tpr_interp\n",
        "\n",
        "    try: precision, recall, _ = precision_recall_curve(df['y_true'], df['y_prob'])\n",
        "    except Exception: precision, recall = np.array([]), np.array([])\n",
        "    if len(recall)>1:\n",
        "        recall_sort_idx = np.argsort(recall)\n",
        "        prec_interp = interpolate_curve(recall[recall_sort_idx], precision[recall_sort_idx], PR_RECALL_GRID)\n",
        "    else:\n",
        "        prec_interp = np.full_like(PR_RECALL_GRID, np.nan)\n",
        "    pr_prec_matrix[run_idx,:] = prec_interp\n",
        "\n",
        "    try:\n",
        "        df['prob_bin'] = pd.cut(df['y_prob'], bins=CAL_PROB_BINS, include_lowest=True, labels=False)\n",
        "        obs_per_bin = [df.loc[df['prob_bin']==b, 'y_true'].mean() if (df['prob_bin']==b).sum()>0 else np.nan\n",
        "                       for b in range(len(CAL_PROB_BINS)-1)]\n",
        "        cal_bin_obs_matrix[run_idx,:] = np.array(obs_per_bin, dtype=float)\n",
        "    except Exception:\n",
        "        cal_bin_obs_matrix[run_idx,:] = np.full(len(CAL_PROB_BINS)-1, np.nan)\n",
        "\n",
        "    # ----------------- Top-K -----------------\n",
        "    try:\n",
        "        topk_df = topk_capture(df['y_true'].values, df['y_prob'].values, ks=TOPK_PERCENTS)\n",
        "        for j,k in enumerate(TOPK_PERCENTS):\n",
        "            topk_matrix[run_idx,j] = topk_df.loc[topk_df['top_%']==k, 'pos_captured_frac'].values[0]\n",
        "        temp = topk_df.copy(); temp['run']=run_idx; temp['seed']=seed; topk_rows.append(temp)\n",
        "    except Exception:\n",
        "        topk_matrix[run_idx,:] = np.nan\n",
        "        topk_rows.append(pd.DataFrame())\n",
        "\n",
        "# ----------------- Aggregation -----------------\n",
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "metrics_df.to_csv(METRICS_PER_RUN_CSV, index=False)\n",
        "\n",
        "def agg_stats(series):\n",
        "    mean = np.nanmean(series)\n",
        "    sd = np.nanstd(series, ddof=1) if np.sum(~np.isnan(series))>1 else np.nan\n",
        "    n = np.sum(~np.isnan(series))\n",
        "    se = sd / math.sqrt(n) if n>0 and not np.isnan(sd) else np.nan\n",
        "    ci95 = 1.96 * se if se is not None else np.nan\n",
        "    return mean, sd, ci95\n",
        "\n",
        "agg_rows = []\n",
        "metrics_to_agg = [\"roc_auc\",\"pr_auc\",\"brier\",\"f1_at_0.5\",\"precision_at_0.5\",\"recall_at_0.5\",\"positive_rate\"]\n",
        "for metric in metrics_to_agg:\n",
        "    mean, sd, ci95 = agg_stats(metrics_df[metric].values)\n",
        "    agg_rows.append({\"stratum\": \"ALL\", \"metric\": metric, \"mean\": mean, \"sd\": sd, \"ci95\": ci95})\n",
        "\n",
        "if stratum_col:\n",
        "    strata = df_orig[stratum_col].dropna().unique().tolist()\n",
        "    for stratum_val in strata:\n",
        "        per_stratum_metrics = []\n",
        "        for run_idx, seed in enumerate(seeds):\n",
        "            np.random.seed(seed)\n",
        "            if SAMPLING_METHOD == 'stratified_bootstrap':\n",
        "                idx_list = []\n",
        "                for cls in df_orig['y_true'].unique():\n",
        "                    cls_idx = df_orig.index[df_orig['y_true']==cls].tolist()\n",
        "                    if len(cls_idx)==0: continue\n",
        "                    s = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
        "                    idx_list.extend(s.tolist())\n",
        "                sampled_idx = np.array(idx_list)\n",
        "            else:\n",
        "                n_take = int(math.ceil(n_total * TEST_SAMPLE_FRACTION))\n",
        "                sampled_idx = np.random.choice(df_orig.index, size=n_take, replace=False)\n",
        "            df_run_stratum = df_orig.loc[sampled_idx][df_orig[stratum_col]==stratum_val]\n",
        "            if len(df_run_stratum)==0:\n",
        "                per_stratum_metrics.append({m: np.nan for m in metrics_to_agg})\n",
        "                continue\n",
        "            try: roc_auc = roc_auc_score(df_run_stratum['y_true'], df_run_stratum['y_prob'])\n",
        "            except Exception: roc_auc = np.nan\n",
        "            try: pr_auc = average_precision_score(df_run_stratum['y_true'], df_run_stratum['y_prob'])\n",
        "            except Exception: pr_auc = np.nan\n",
        "            brier = brier_score_loss(df_run_stratum['y_true'], df_run_stratum['y_prob'])\n",
        "            y_pred = (df_run_stratum['y_prob']>=0.5).astype(int)\n",
        "            per_stratum_metrics.append({\n",
        "                \"roc_auc\": roc_auc, \"pr_auc\": pr_auc, \"brier\": brier,\n",
        "                \"f1_at_0.5\": f1_score(df_run_stratum['y_true'], y_pred, zero_division=0),\n",
        "                \"precision_at_0.5\": precision_score(df_run_stratum['y_true'], y_pred, zero_division=0),\n",
        "                \"recall_at_0.5\": recall_score(df_run_stratum['y_true'], y_pred, zero_division=0),\n",
        "                \"positive_rate\": float(df_run_stratum['y_true'].mean())\n",
        "            })\n",
        "        per_stratum_df = pd.DataFrame(per_stratum_metrics)\n",
        "        for metric in metrics_to_agg:\n",
        "            mean, sd, ci95 = agg_stats(per_stratum_df[metric].values)\n",
        "            agg_rows.append({\"stratum\": str(stratum_val), \"metric\": metric, \"mean\": mean, \"sd\": sd, \"ci95\": ci95})\n",
        "\n",
        "agg_df = pd.DataFrame(agg_rows)\n",
        "agg_df.to_csv(METRICS_AGG_CSV, index=False)\n",
        "\n",
        "# ----------------- Top-K aggregation -----------------\n",
        "topk_all = pd.concat([t.assign(run=int(t['run'].iloc[0])) if not t.empty else pd.DataFrame() for t in topk_rows], ignore_index=True, sort=False)\n",
        "if not topk_all.empty: topk_all.to_csv(TOPK_PER_RUN_CSV, index=False)\n",
        "topk_agg = [{\"top_%\": k, \"mean_frac\": agg_stats(topk_matrix[:,j])[0],\n",
        "             \"sd\": agg_stats(topk_matrix[:,j])[1], \"ci95\": agg_stats(topk_matrix[:,j])[2]}\n",
        "            for j,k in enumerate(TOPK_PERCENTS)]\n",
        "pd.DataFrame(topk_agg).to_csv(TOPK_AGG_CSV, index=False)\n",
        "\n",
        "# ----------------- Plots (ROC, PR, Top-K, Reliability) -----------------\n",
        "# Convert to arrays\n",
        "roc_arr = np.array(roc_list)\n",
        "pr_arr = np.array(pr_list)\n",
        "cal_arr = np.array(cal_list)\n",
        "topk_arr = np.array(topk_list)\n",
        "metrics_df = pd.DataFrame(metrics_all)\n",
        "\n",
        "# ----------------- Plotting with ribbons -----------------\n",
        "ROC_FPR_GRID = np.linspace(0,1,100)\n",
        "PR_RECALL_GRID = np.linspace(0,1,100)\n",
        "TOPK_PERCENTS = np.linspace(0.01,1,100)\n",
        "CAL_PROB_BINS = np.linspace(0,1,11)  # 10 bins\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ---------- ROC Curve ----------\n",
        "mean_tpr = np.nanmean(roc_arr, axis=0)\n",
        "sd_tpr = np.nanstd(roc_arr, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(ROC_FPR_GRID, mean_tpr, label=f\"Mean ROC (n={NUM_RUNS})\", linewidth=2)\n",
        "plt.fill_between(ROC_FPR_GRID, mean_tpr - sd_tpr, mean_tpr + sd_tpr, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', color='gray', label='Random')\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=18)\n",
        "plt.ylabel('True Positive Rate', fontsize=18)\n",
        "plt.title('ROC Curve (mean ± SD)', fontsize=18)\n",
        "plt.legend(loc='lower right', fontsize=18)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'roc_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ---------- PR Curve ----------\n",
        "mean_prec = np.nanmean(pr_arr, axis=0)\n",
        "sd_prec = np.nanstd(pr_arr, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(PR_RECALL_GRID, mean_prec, label=f\"Mean PR (n={NUM_RUNS})\", linewidth=2)\n",
        "plt.fill_between(PR_RECALL_GRID, mean_prec - sd_prec, mean_prec + sd_prec, alpha=0.2)\n",
        "\n",
        "plt.xlabel('Recall', fontsize=18)\n",
        "plt.ylabel('Precision', fontsize=18)\n",
        "plt.title('Precision-Recall Curve (mean ± SD)', fontsize=18)\n",
        "plt.legend(loc='upper right', fontsize=18)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'pr_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ---------- Top-K Capture Plot ----------\n",
        "n_topk = topk_matrix.shape[1]\n",
        "x_vals = TOPK_PERCENTS if 'TOPK_PERCENTS' in globals() and len(TOPK_PERCENTS)==n_topk else np.linspace(0, 50, n_topk)\n",
        "mean_topk = np.nanmean(topk_matrix, axis=0)\n",
        "sd_topk = np.nanstd(topk_matrix, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x_vals, mean_topk, marker='o', color='tab:blue', linewidth=2, label='Mean Top-K capture')\n",
        "plt.fill_between(x_vals, mean_topk - sd_topk, mean_topk + sd_topk, color='tab:blue', alpha=0.2, label='± SD')\n",
        "\n",
        "plt.xlabel('Top-k percent of highest risk area', fontsize=18)\n",
        "plt.ylabel('Fraction of fires captured', fontsize=18)\n",
        "plt.title('Top-K Capture (mean ± SD)', fontsize=18)\n",
        "plt.ylim(0,1)\n",
        "plt.xlim(0,50)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend(loc='lower right', fontsize=18)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'topk_curve_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ---------- Reliability / Calibration ----------\n",
        "cal_bin_centers = (CAL_PROB_BINS[:-1] + CAL_PROB_BINS[1:]) / 2.0\n",
        "mean_bin_obs = np.nanmean(cal_arr, axis=0)\n",
        "sd_bin_obs = np.nanstd(cal_arr, axis=0, ddof=1)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(cal_bin_centers, mean_bin_obs, marker='o', linewidth=2, label='Mean calibration')\n",
        "plt.fill_between(cal_bin_centers, mean_bin_obs - sd_bin_obs, mean_bin_obs + sd_bin_obs, alpha=0.2)\n",
        "plt.plot([0,1],[0,1], linestyle='--', color='gray', label='Perfect')\n",
        "\n",
        "plt.xlabel('Predicted probability (bin center)', fontsize=16)\n",
        "plt.ylabel('Observed frequency', fontsize=16)\n",
        "plt.title('Reliability Diagram (mean ± SD)', fontsize=16)\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,'reliability_mean_sd.png'), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ----------------- Summary Text -----------------\n",
        "report_lines = [\n",
        "    \"Region-transfer / Temporal-style Multi-run Evaluation\",\n",
        "    f\"File: {csv_path}\",\n",
        "    f\"Total original samples: {n_total}, Overall positive rate: {pos_rate_total:.4f}\",\n",
        "    f\"Sampling method: {SAMPLING_METHOD}\",\n",
        "    f\"Number of runs (K): {K_RUNS}\",\n",
        "    f\"Base seed (document for reproducibility): {BASE_SEED}\",\n",
        "    f\"Seeds used: {seeds}\",\n",
        "    \"\",\n",
        "    f\"Metrics per-run saved to: {METRICS_PER_RUN_CSV}\",\n",
        "    f\"Aggregated metrics saved to: {METRICS_AGG_CSV} (mean ± sd ± 95%CI)\",\n",
        "    f\"Top-K per-run saved to: {TOPK_PER_RUN_CSV}\",\n",
        "    f\"Top-K aggregated saved to: {TOPK_AGG_CSV}\",\n",
        "    f\"ROC plot (mean ± SD): {OUT_DIR / 'roc_curve_mean_sd.png'}\",\n",
        "    f\"PR plot (mean ± SD): {OUT_DIR / 'pr_curve_mean_sd.png'}\",\n",
        "    f\"Reliability plot (mean ± SD): {OUT_DIR / 'reliability_mean_sd.png'}\",\n",
        "    f\"Top-K plot (mean ± SD): {OUT_DIR / 'topk_curve_mean_sd.png'}\",\n",
        "]\n",
        "with open(SUMMARY_TXT, 'w') as f:\n",
        "    f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "print(\"\\n\".join(report_lines))\n",
        "print(\"Outputs written to:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use gdown to download the folder\n",
        "!gdown \"https://drive.google.com/drive/u/0/folders/1BjUySwa9W__GIX4arSDQHRHT8brUGjap\" --folder"
      ],
      "metadata": {
        "id": "uCvzyGYge-2i",
        "outputId": "97945090-5a86-4dc8-80ee-07977adbb507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uCvzyGYge-2i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1rKgmkgPXAdch0kSCMlG5BiABuTHVq15r results_region_transfer\n",
            "Processing file 1MAHhVOjdidwadwULttFfOigE0vMVWc48 metrics_per_run.csv\n",
            "Processing file 1Vzv-05tDwKm09ftsZcmEfNrLmVmKxKHo pr_curve_mean_sd.png\n",
            "Processing file 13U6alSSJgDx1veMaubIOFvVTpV6zDUf0 reliability_mean_sd.png\n",
            "Processing file 1DypI6QnzOaGZabZClmWax4WuEKaOqglZ rf_model_42.joblib\n",
            "Processing file 1EvoUeHw8HXHeJg1IWCKOFvNxpzDysRrq rf_model_43.joblib\n",
            "Processing file 19wfT4MGPmiKcaD35NFhggCWINn4mW_6g rf_model_44.joblib\n",
            "Processing file 1U5nUAdQtDEJuVv2GaELyVtw1MhIsSMw3 rf_model_45.joblib\n",
            "Processing file 1VYQKWS63YaAEuEDpIRUHrvaIUJg_N46B rf_model_46.joblib\n",
            "Processing file 1dNqN9B8qUK9PhFsTp9OliZ4OOLlo5VvY rf_model_47.joblib\n",
            "Processing file 1DpLUEKAoYzWlDC5Uqrg1pJvbucK8wzyO rf_model_48.joblib\n",
            "Processing file 1cHajbGu7cavQN8vSU_YzZRNaMSnOmgtw rf_model_49.joblib\n",
            "Processing file 1zg7rYGpfc1oHyiELqDoPJvl6kG4W7n7H rf_model_50.joblib\n",
            "Processing file 1zsK_JXDJ07v6dKCjWqcE8nleKTeo6tYe rf_model_51.joblib\n",
            "Processing file 1gsy497dEpa8v-GMPKraLPpyTbKVQr0zC roc_curve_mean_sd.png\n",
            "Processing file 1JcCVeNnBtuJkO_OQkHS0cR7vT7VagpZl summary_report_multi_run.txt\n",
            "Processing file 1Ms1E-z0fjRBv9hKCOKxJyuqNESBPs3M8 topk_curve_mean_sd.png\n",
            "Processing file 1IPnC2DCoPs7yqzOTtw0qAHZ4n0al6ri8 topk_per_run.csv\n",
            "Retrieving folder 1_jQ5_gjan4Gx8vtLF6B3SypHxCt8Cwv8 results_region_transfer_V1\n",
            "Processing file 13e0CO63hRef8PKF7DnB3fvXlnaoT0tLC calibration_curve_2025.png\n",
            "Processing file 1DMrloJTd6qSHjJB-lCbxcrIDM1y_cm0K pr_curve_2025.png\n",
            "Processing file 1Wv6TMhA62mY3dRhKcmkcL6FLLlbEP1ii rf_model_2025.joblib\n",
            "Processing file 1C0zKr3vhJxeQbC6DIgzzpjBMoM7fhX7O roc_curve_2025.png\n",
            "Processing file 1ULS8zKZ5c-lZdR-XiUtIU-qP2D2vvXxi summary_report_2025.txt\n",
            "Processing file 1OnkdO4UGHz2M5gxx1mJqKa31_OXctkp0 test2025_metrics.csv\n",
            "Processing file 1mdxNwhmjramkjeVgAtOd1MybH_7CW7wo topk_capture_2025.csv\n",
            "Processing file 1grBl1lBmgBb_DRHqT74QcgH_blpryNkq topk_capture_2025.png\n",
            "Retrieving folder 1uCERv0-Xop-vdTzNs3vuRCY7iFmgkOwW results_region_transfer_V2\n",
            "Processing file 1T4iw3TMX0oPi5oVjq7tuRRJ4qGlFejbW metrics_aggregated_by_stratum.csv\n",
            "Processing file 1CIP8W4iVxhLN-wiBKwQbaYh-mmL_5gTr metrics_per_run.csv\n",
            "Processing file 1ofHQ2RWlU9O5MXPGwpB3cW6fDyF_h3Ra RT_G_pr_curve_mean_sd.png\n",
            "Processing file 16NioX57Vw1Yyx1Ov2cO_u0yoccRPXw4R RT_G_reliability_mean_sd.png\n",
            "Processing file 1uszWnnK67t0E4CBSpZt0E7grrhdFkO1_ RT_G_roc_curve_mean_sd.png\n",
            "Processing file 1v5IZETDJCcNFRLagR9-sqcx1RVk7L1_o RT_G_topk_curve_mean_sd.png\n",
            "Processing file 1n6XYjNLS4V7JpqcJcDfQsLEKl3Byj_ms summary_report_multi_run.txt\n",
            "Processing file 1nof7piI9dZ4ONaFhbVED18rx6L7p_QvL topk_aggregated.csv\n",
            "Processing file 10XYsOIoSsEi45MvY8kQwVNota_yt3_Pj topk_per_run.csv\n",
            "Retrieving folder 1h8oSmFLlXHtfg1MVAOjL7wD2qORZtmKv results_region_transfer_V3\n",
            "Processing file 13MmTS7Ixick9nHXeyL1AtI6Pv2mpjBhR metrics_aggregated_by_stratum.csv\n",
            "Processing file 1K9rbpVKqM_SLQa9yvUH8QuVnATfYpwUp metrics_per_run.csv\n",
            "Processing file 1fO24rLG_3IgPvf_93uGeX16C84Pd3Zuq rf_model_42.joblib\n",
            "Processing file 1bAP4dTPCdwdJppKNxoZunQW2BMr7OaWN rf_model_43.joblib\n",
            "Processing file 1RyFmDVLHTjELQyzT4lY-rXlFae34sZh5 rf_model_44.joblib\n",
            "Processing file 1n-R6TG93J-Ka0QfY7L93bmu2i-YVWEh2 rf_model_45.joblib\n",
            "Processing file 18Hi5E6-WrRkg2zIHw6EBom9nyEJe52Sb rf_model_46.joblib\n",
            "Processing file 1DreMNjxOk7Hk5R2yoKf0Inb8hWsbmpEA rf_model_47.joblib\n",
            "Processing file 13DXTlXGofYFHf9-uBYAO5xKLPPRYC6ie rf_model_48.joblib\n",
            "Processing file 1Cl5AdvHarsVJut72GFJAelpHcmH2nzu4 rf_model_49.joblib\n",
            "Processing file 1LoqjZpIjRJgeD6af6rJxelNPHhWDNAYH rf_model_50.joblib\n",
            "Processing file 18bGhuhP71AHrtR6WDTj0mWc7qsF1Cty- rf_model_51.joblib\n",
            "Processing file 18RJP2ou2-R7cK7kn1A_OSxDVanmZH94d RT_G_pr_curve_mean_sd.png\n",
            "Processing file 1nmasfxgPT-Qnjk83cEciDAAdJH9Glnb1 RT_G_reliability_mean_sd.png\n",
            "Processing file 1JFCtt6V5CB1B7fiLYmbBLU4ihB58E_99 RT_G_roc_curve_mean_sd.png\n",
            "Processing file 1_TWnRY5z8yiI2PNFw_SotblfuPVVyL2r RT_G_topk_curve_mean_sd.png\n",
            "Processing file 1_O0A0ZryhEKnFVJ0mZJObqYxkSXgiiFz summary_report_multi_run.txt\n",
            "Processing file 1yXgwLOMsUrMctfgWONc3Oqf55VNSxHlc topk_aggregated.csv\n",
            "Processing file 1hL4wHpQRB_-ybRZ4ET7ijbfgc5CM4JOY topk_per_run.csv\n",
            "Retrieving folder 1BE0P_XWBhpwkiH64Fd0jWOpj09GAZ3Z_ results_region_transfer_V4\n",
            "Processing file 1HgLeDWo4yG_wQUfIcDiIxFpwMWRDnpD7 metrics_aggregated_by_stratum.csv\n",
            "Processing file 1KA4SvZ3F9l27bdKy1xHzhW7N3Fzp2GzJ metrics_per_run.csv\n",
            "Processing file 1x1kDJOoGGamS-Wcved2c1Dtw-Q1Bxa8F rf_model_42.joblib\n",
            "Processing file 1dWnMgrMlY7uuEfQ43bVWDx8J3w9GuKht rf_model_43.joblib\n",
            "Processing file 1tKw8KhOKnQGffvIucgBxsejQRANgIiOK rf_model_44.joblib\n",
            "Processing file 1KYzsGApICLp6gU9vCwE1Pvh0gMFG8AlA rf_model_45.joblib\n",
            "Processing file 15qZBjvxn4VPlcvp0K1Q0JOiNJd3AP0YY rf_model_46.joblib\n",
            "Processing file 15kWdOSRP_xzaQn_OFraviTbMUrf10FE4 rf_model_47.joblib\n",
            "Processing file 13fK7dq52unMyP6o3ewgNydHA9uuQOD8y rf_model_48.joblib\n",
            "Processing file 16eLY7UG0GGw-e_UrIVjEjiHlji-dgAEa rf_model_49.joblib\n",
            "Processing file 1tz8atl__t2xYvIt8REBp5BIAkpEeH74I rf_model_50.joblib\n",
            "Processing file 1A8cqHsvVe3QfF4Yhtqmipnneqj_k03tN rf_model_51.joblib\n",
            "Processing file 1bXUv34n-7wbAHACTrp_I34pi8_zz9hyv RT_G_pr_curve_mean_sd.png\n",
            "Processing file 1cAL_xw0tdtn6QJUxjgJ5ywMc3Jb1p29X RT_G_reliability_mean_sd.png\n",
            "Processing file 1czfPJDJBmhfRYKTZk0N3g0yYzrV1X-np RT_G_roc_curve_mean_sd.png\n",
            "Processing file 102wz0Bd49ecoyFtptJsimbECvTs8OTmq RT_G_topk_curve_mean_sd.png\n",
            "Processing file 1A5KBvFPwCAHl2ys2eDSBW3P35xbM0Yso summary_report_multi_run.txt\n",
            "Processing file 1qKHRROOv7Uf0GjsjOnMgVVGWXgNzjrxf topk_aggregated.csv\n",
            "Processing file 168NwSGfhk9IWjq_--hrZSSPJgDXNjMv7 topk_per_run.csv\n",
            "Retrieving folder 1l4Td-PC3msE0STKmMeFu9kRTaSIrFtyX results_region_transfer_V5\n",
            "Processing file 1Pym_73v9dvaTqu--Nh4Ikobvbxt_NK6m metrics_aggregated_by_stratum.csv\n",
            "Processing file 1nnOww_U2pJFqrnJuvm7VhCA6LBtAV6zJ metrics_per_run.csv\n",
            "Processing file 1QRGTU-ZFrubcmr1tlzpatETkwXNuFmO7 pr_curve_mean_sd.png\n",
            "Processing file 1i_hrV5Wem-DPMsvjEzhnJYAwn5OYMy22 reliability_mean_sd.png\n",
            "Processing file 1-wFzo-jfGSK2l9ejlsh0jT2cbg9vgSc7 rf_model_42.joblib\n",
            "Processing file 1i0qTl62hD_9Utfv0FwXNtH47m53ZVBrl rf_model_43.joblib\n",
            "Processing file 1VdQB7LCxCLL0ekTuJXgBFKSzddY3WkdK rf_model_44.joblib\n",
            "Processing file 1vz-TbnmBOUe4Dw7tyyn_htPsv_JN7LAh rf_model_45.joblib\n",
            "Processing file 1q6uqE2aOdXAOixcGMdbqR8ylH9RqUlVz rf_model_46.joblib\n",
            "Processing file 1Qa7voqb1q3OYeYvhulRmcKNWZATRfSJe rf_model_47.joblib\n",
            "Processing file 1_FZqEKOYQY2w75-mpeQuARKsyFWxFqVj rf_model_48.joblib\n",
            "Processing file 11Vu7dlyE7nWL-QQcAjdvCS0PGnWSYYtN rf_model_49.joblib\n",
            "Processing file 1o9BX-mRmqZ1ICtoakWs4PPUtjbfyzIVy rf_model_50.joblib\n",
            "Processing file 1pmDyVUHoFILJc7TOdo58byDeb8wQzF7j rf_model_51.joblib\n",
            "Processing file 1tVJwH4zbCHRsmidYPXiFdE72I1kNjNZq roc_curve_mean_sd.png\n",
            "Processing file 1W_eaFOghZMqI7zGUv69U831qGN1UMNmZ summary_report_multi_run.txt\n",
            "Processing file 156wYOfUuFYp2F4aueM_b0kpWzcdXhKCB topk_aggregated.csv\n",
            "Processing file 1PyIQLBr1rJszgVv_FhjzJA0bhqttKrTv topk_curve_mean_sd.png\n",
            "Processing file 14FHRHKBGTo8-XRVky0ZjRqM1Qonomkpu topk_per_run.csv\n",
            "Retrieving folder 1AXcJz8x9zorl8IIV3FDTmyP7rXBkOs55 Temporal_Split_V1\n",
            "Processing file 1lhPtKfxCtSj-o1nVyRpK9JlrvZDxHmNA pr_curve_2025.png\n",
            "Processing file 13Qp9ICK-dqCvrAe1HSUqSLRdgTPJuwbM reliability_2025.png\n",
            "Processing file 1wNG3Atc90GWCtKDqXqacsv8Y3wx00UqE risk_exposure_quadrants_2025.csv\n",
            "Processing file 1nVNPUBZA2WnZCCbKlRE-IfjfO8LZltjB risk_exposure_scatter_2025.png\n",
            "Processing file 1edmhepQS91GuH1-08oWs_ib7S3W87d_w roc_curve_2025.png\n",
            "Processing file 1_68cvChKOlhJG4r1v90NrR8iBF-GmWPF shap_summary_bar_2025.png\n",
            "Processing file 1rVx7XKRubKnVxQXtISrGua00gOsGBiiQ summary_report_2025.txt\n",
            "Processing file 1SVXVEYkIDtiMy_oK6BKvxxlImdyafDWA test2025_metrics.csv\n",
            "Processing file 1ZBVKcg7dznq6OUivGiyxfXNFPT064qDJ topk_capture_2025.csv\n",
            "Processing file 1WYhPtmDZaIGM78O5PH_rOcTmRYEiMrrx topk_curve_2025.png\n",
            "Retrieving folder 18jzgGWE9l-kzD-8nCf-g45FFSgaRYfAS Temporal_Split_V2\n",
            "Processing file 12RvDB8fXS8s5ZWfylGoT_X5xciwPmZZk metrics_aggregated_by_stratum.csv\n",
            "Processing file 1zGjZIs7AeTRa5rPI_mm3EU7Db-NqDvhs metrics_per_run.csv\n",
            "Processing file 1yea_FceiEwzw32a1ZklkQmQcuMSRXr1e summary_report_multi_run.txt\n",
            "Processing file 1r22Y6zvI-P7WwotQATrpvU2SFPIB4nCE topk_aggregated.csv\n",
            "Processing file 1XurmAd6XlsHbSyXS1E5LtDUwEHXmtub2 topk_per_run.csv\n",
            "Processing file 1uKJYpheyAQH95Ifv05ujRZnFB1Q3U71h TS_G_pr_curve_mean_sd.png\n",
            "Processing file 1xsWG8wTLvQubEtTe9m89rYxHXFuvs0FO TS_G_reliability_mean_sd.png\n",
            "Processing file 1CM2ypDaDNEJCo7qZzvsX_opzgsypit9u TS_G_roc_curve_mean_sd.png\n",
            "Processing file 1tqdXk_NrdOQm2zsOxB8YLAMCmu_4vh6w TS_G_shap_summary_bar.png\n",
            "Processing file 1PlLIuSV7cogv-pyPf7JrGKUFGcu4_5Z_ TS_G_topk_curve_mean_sd.png\n",
            "Retrieving folder 15uIvs4ymlVfazVdVRnreNXTerP8oMJgi Temporal_Split_V3\n",
            "Processing file 1-f8FeA4XVVG9aXMa2pRZbGn7jlaC0tCq metrics_aggregated_by_stratum.csv\n",
            "Processing file 1QmbPKk1ji2ceniRz_tvmjMjUARFtrWY2 metrics_per_run.csv\n",
            "Processing file 1XdWgg4ioY4PgAQqcmt7PqlRm4ayOoDx1 shap_summary_bar.png\n",
            "Processing file 1zh1OcAqc-3-0vA_CiiVRLhcbn_Cd21zf summary_report_multi_run.txt\n",
            "Processing file 1izH9ZivNiHMiEjVFolNH0EIw8rTjB5fO topk_aggregated.csv\n",
            "Processing file 1Sq6YH3s9jmc0-waYgRSBIMPp_0FzLhFf topk_per_run.csv\n",
            "Processing file 1mYXnKiBmbI46NFIVh26TMOIpzMuNYGPW TS_F_pr_curve_mean_sd.png\n",
            "Processing file 1-eUBSLuZzsHaXpLjeLs4K-NnFPbUwn8C TS_F_reliability_mean_sd.png\n",
            "Processing file 1MHSFd3Q8a982Wlb59UQaFDWmu9hWqQV6 TS_F_roc_curve_mean_sd.png\n",
            "Processing file 1S6pDtvq2elOWAU_oXatVBEwr_4spx3eW TS_F_topk_curve_mean_sd.png\n",
            "Retrieving folder 17Nihau_nlGFnlp5rPPLBdCXi9y-vxpj5 Temporal_Split_V4\n",
            "Processing file 1OCq4eNjzyLagFyo18fG1sm96qVgA1sfv metrics_aggregated_by_stratum.csv\n",
            "Processing file 1mdxGYPeRF9oUd5pZ7HH1ad-sTgyZHb98 metrics_per_run.csv\n",
            "Processing file 1DZtFQvhO5S18NLKgnCkSCkQUgFurkLUn summary_report_multi_run.txt\n",
            "Processing file 1WVweWsjAhJDWd_iNduRuNUNeva8MeLWt topk_aggregated.csv\n",
            "Processing file 1AYUgppAGgVbgkfbEex7SPFgbCFq9DtRK topk_per_run.csv\n",
            "Processing file 13QGU0hkvCVLqKtbOrXW21KpcflaxYXDH TS_G_pr_curve_mean_sd.png\n",
            "Processing file 1h_0WoAJe9KzYBFbJgBp5-n8TgXqmfzpn TS_G_reliability_mean_sd.png\n",
            "Processing file 1v3VZDkelJPQesUwgq3XF2ZD5yyfFyGaf TS_G_roc_curve_mean_sd.png\n",
            "Processing file 1yDVQO5pMZKRoaxurc1QXmtRPSsX31qmA TS_G_shap_summary_bar.png\n",
            "Processing file 1D_VmcuxZRsQwjiss0icGKKRHOIWZGxLN TS_G_topk_curve_mean_sd.png\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MAHhVOjdidwadwULttFfOigE0vMVWc48\n",
            "To: /content/Results/results_region_transfer/metrics_per_run.csv\n",
            "100% 1.23k/1.23k [00:00<00:00, 4.71MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Vzv-05tDwKm09ftsZcmEfNrLmVmKxKHo\n",
            "To: /content/Results/results_region_transfer/pr_curve_mean_sd.png\n",
            "100% 50.2k/50.2k [00:00<00:00, 67.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13U6alSSJgDx1veMaubIOFvVTpV6zDUf0\n",
            "To: /content/Results/results_region_transfer/reliability_mean_sd.png\n",
            "100% 80.0k/80.0k [00:00<00:00, 95.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DypI6QnzOaGZabZClmWax4WuEKaOqglZ\n",
            "To: /content/Results/results_region_transfer/rf_model_42.joblib\n",
            "100% 1.34M/1.34M [00:00<00:00, 123MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EvoUeHw8HXHeJg1IWCKOFvNxpzDysRrq\n",
            "To: /content/Results/results_region_transfer/rf_model_43.joblib\n",
            "100% 1.31M/1.31M [00:00<00:00, 157MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19wfT4MGPmiKcaD35NFhggCWINn4mW_6g\n",
            "To: /content/Results/results_region_transfer/rf_model_44.joblib\n",
            "100% 1.29M/1.29M [00:00<00:00, 133MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1U5nUAdQtDEJuVv2GaELyVtw1MhIsSMw3\n",
            "To: /content/Results/results_region_transfer/rf_model_45.joblib\n",
            "100% 1.31M/1.31M [00:00<00:00, 141MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VYQKWS63YaAEuEDpIRUHrvaIUJg_N46B\n",
            "To: /content/Results/results_region_transfer/rf_model_46.joblib\n",
            "100% 1.34M/1.34M [00:00<00:00, 134MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dNqN9B8qUK9PhFsTp9OliZ4OOLlo5VvY\n",
            "To: /content/Results/results_region_transfer/rf_model_47.joblib\n",
            "100% 1.30M/1.30M [00:00<00:00, 98.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DpLUEKAoYzWlDC5Uqrg1pJvbucK8wzyO\n",
            "To: /content/Results/results_region_transfer/rf_model_48.joblib\n",
            "100% 1.32M/1.32M [00:00<00:00, 105MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cHajbGu7cavQN8vSU_YzZRNaMSnOmgtw\n",
            "To: /content/Results/results_region_transfer/rf_model_49.joblib\n",
            "100% 1.30M/1.30M [00:00<00:00, 124MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zg7rYGpfc1oHyiELqDoPJvl6kG4W7n7H\n",
            "To: /content/Results/results_region_transfer/rf_model_50.joblib\n",
            "100% 1.30M/1.30M [00:00<00:00, 94.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zsK_JXDJ07v6dKCjWqcE8nleKTeo6tYe\n",
            "To: /content/Results/results_region_transfer/rf_model_51.joblib\n",
            "100% 1.32M/1.32M [00:00<00:00, 163MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gsy497dEpa8v-GMPKraLPpyTbKVQr0zC\n",
            "To: /content/Results/results_region_transfer/roc_curve_mean_sd.png\n",
            "100% 90.3k/90.3k [00:00<00:00, 65.6MB/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}